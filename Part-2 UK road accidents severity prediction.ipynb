{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Predict the Accident Severity.\n",
    "Accidents are one of the leading causes of death worldwide. Due to the increase in population and motorization, the number of accidents worldwide has been increasing. Better prediction of the severity of accidents is essential to improve the safety performance of road traffic systems and reduce such unfortunate accidents.\n",
    "\n",
    "**For this we are using accident data from uk goverment website \"https://www.gov.uk/government/statistical-data-sets/reported-road-accidents-vehicles-and-casualties-tables-for-great-britain\"** **\"Road Saftey data-accident 2019.csv\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the base libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Loading the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the datasets.\n",
    "trainset = pd.read_csv(\"train.csv\").drop(['Unnamed: 0'],axis=1) \n",
    "testset = pd.read_csv(\"test.csv\").drop(['Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>Night</th>\n",
       "      <th>Darkness with lights unlit</th>\n",
       "      <th>...</th>\n",
       "      <th>March</th>\n",
       "      <th>May</th>\n",
       "      <th>November</th>\n",
       "      <th>October</th>\n",
       "      <th>September</th>\n",
       "      <th>Urban</th>\n",
       "      <th>Flood</th>\n",
       "      <th>Frost</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Wet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Severity  Number_of_Casualties  Monday  Saturday  Sunday  Thursday  \\\n",
       "0            Slight                     1     0.0       0.0     0.0       0.0   \n",
       "1            Slight                     1     0.0       0.0     1.0       0.0   \n",
       "2            Slight                     1     0.0       0.0     0.0       0.0   \n",
       "3            Slight                     1     1.0       0.0     0.0       0.0   \n",
       "4            Slight                     1     0.0       0.0     0.0       0.0   \n",
       "\n",
       "   Tuesday  Wednesday  Night  Darkness with lights unlit  ...  March  May  \\\n",
       "0      1.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "1      0.0        0.0    0.0                         0.0  ...    0.0  1.0   \n",
       "2      1.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "3      0.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "4      1.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "\n",
       "   November  October  September  Urban  Flood  Frost  Snow  Wet  \n",
       "0       0.0      0.0        0.0    1.0    0.0    0.0   0.0  1.0  \n",
       "1       0.0      0.0        0.0    0.0    0.0    0.0   0.0  0.0  \n",
       "2       0.0      0.0        0.0    1.0    0.0    0.0   0.0  1.0  \n",
       "3       0.0      0.0        0.0    1.0    0.0    0.0   0.0  0.0  \n",
       "4       0.0      0.0        0.0    1.0    0.0    0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying results\n",
    "trainset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accident_Severity</th>\n",
       "      <th>Number_of_Casualties</th>\n",
       "      <th>Monday</th>\n",
       "      <th>Saturday</th>\n",
       "      <th>Sunday</th>\n",
       "      <th>Thursday</th>\n",
       "      <th>Tuesday</th>\n",
       "      <th>Wednesday</th>\n",
       "      <th>Night</th>\n",
       "      <th>Darkness with lights unlit</th>\n",
       "      <th>...</th>\n",
       "      <th>March</th>\n",
       "      <th>May</th>\n",
       "      <th>November</th>\n",
       "      <th>October</th>\n",
       "      <th>September</th>\n",
       "      <th>Urban</th>\n",
       "      <th>Flood</th>\n",
       "      <th>Frost</th>\n",
       "      <th>Snow</th>\n",
       "      <th>Wet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Serious</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Slight</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Accident_Severity  Number_of_Casualties  Monday  Saturday  Sunday  Thursday  \\\n",
       "0            Slight                     1     0.0       0.0     0.0       0.0   \n",
       "1            Slight                     1     0.0       0.0     0.0       0.0   \n",
       "2           Serious                     1     1.0       0.0     0.0       0.0   \n",
       "3            Slight                     1     0.0       0.0     0.0       0.0   \n",
       "4            Slight                     1     0.0       0.0     1.0       0.0   \n",
       "\n",
       "   Tuesday  Wednesday  Night  Darkness with lights unlit  ...  March  May  \\\n",
       "0      1.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "1      0.0        1.0    0.0                         0.0  ...    0.0  0.0   \n",
       "2      0.0        0.0    1.0                         0.0  ...    0.0  0.0   \n",
       "3      1.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "4      0.0        0.0    0.0                         0.0  ...    0.0  0.0   \n",
       "\n",
       "   November  October  September  Urban  Flood  Frost  Snow  Wet  \n",
       "0       0.0      0.0        0.0    0.0    0.0    0.0   1.0  1.0  \n",
       "1       0.0      0.0        0.0    1.0    0.0    0.0   0.0  0.0  \n",
       "2       0.0      0.0        1.0    1.0    0.0    0.0   0.0  0.0  \n",
       "3       0.0      0.0        0.0    1.0    0.0    0.0   0.0  1.0  \n",
       "4       0.0      0.0        0.0    0.0    0.0    0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying results\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17607, 34)\n",
      "(5826, 34)\n"
     ]
    }
   ],
   "source": [
    "#Checking the dimensions of the datasets\n",
    "print (trainset.shape)\n",
    "print (testset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop labels for training set, but keep all others\n",
    "#creating separate arrays for the predictors (`Xtrain`) and for the target (`ytrain`):\n",
    "Xtrain = trainset.drop(\"Accident_Severity\", axis=1)\n",
    "ytrain = trainset[\"Accident_Severity\"].copy()\n",
    "\n",
    "#creating separate arrays for the testset (`Xtest`) and for the target (`ytest`):\n",
    "Xtest = testset.drop(\"Accident_Severity\", axis=1)\n",
    "ytest = testset[\"Accident_Severity\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Baseline Models\n",
    "We'll use a majority class classifier as a baseline, i.e., we will find out what is the most common class label in the training set and always output it as a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slight     14020\n",
       "Serious     3365\n",
       "Fatal        222\n",
       "Name: Accident_Severity, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the number of instances belonging to each class\n",
    "trainset[\"Accident_Severity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline classifier will output \"Slight\" for all predictions. We will use macro-averaging in this project (precision, recall and F-score are evaluated in each class separately and then averaged across classes).\n",
    "\n",
    "So if we apply the baseline classifier to all of the training set. \n",
    "\n",
    "For the \"Slight\" label, the accuracy measures will be:\n",
    "\n",
    "* Precision: 14020/17607 = 0.796\n",
    "\n",
    "* Recall: 14020/14020 = 1.0\n",
    "\n",
    "* F-score: 2/(1/precision + 1/recall) = 0.886\n",
    "\n",
    "The averages of the three classes, i.e. the eventual baseline scores:\n",
    "\n",
    "* **Precision: 0.2653**\n",
    "\n",
    "* **Recall: 0.3333**\n",
    "\n",
    "* **F-score: 0.2953**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Logistics Regression Model Using Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:615: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 598, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 443, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.29552808        nan 0.29552808        nan 0.29552808\n",
      "        nan 0.29552808        nan 0.29552808]\n",
      "  warnings.warn(\n",
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:922: UserWarning: One or more of the train scores are non-finite: [       nan 0.29552808        nan 0.29552808        nan 0.29552808\n",
      "        nan 0.29552808        nan 0.29552808]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 9.335018157958984 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Importing Grid search library functions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# specifying the hyperparameter\n",
    "grid={\"C\":[100,10,1.0,0.1,0.01],\"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "\n",
    "#loading LogisticRegression\n",
    "logreg=LogisticRegression()\n",
    "\n",
    "# 5 fold cross validation used\n",
    "logreg_cv=GridSearchCV(logreg,grid,cv=5,\n",
    "                      scoring='f1_macro', \n",
    "                      return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "logreg_cv.fit(Xtrain,ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying best estimators for logistic regression\n",
    "logreg_cv.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Grid-search we found that Best estimator for logistic regression is (c=100)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29552808431814326"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best score\n",
    "logreg_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The achieve the F-score of **0.295**,which is not a good score and it is equal to the basline model result which was **0.295**. One of the reason for low score could be imbalanced data or may be this model is not suited for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displyaing rank of best performing model\n",
    "logreg_cv.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will store the best model in each split for future use.\n",
    "log_split_test_scores = []\n",
    "for x in range(5):\n",
    "    \n",
    "# extract f-score of the best model (index=1) from each of the 5 splits \n",
    "    val = logreg_cv.cv_results_[f\"split{x}_test_score\"][1] \n",
    "    log_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan {'C': 100, 'penalty': 'l1'}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 100, 'penalty': 'l2'}\n",
      "nan nan {'C': 10, 'penalty': 'l1'}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 10, 'penalty': 'l2'}\n",
      "nan nan {'C': 1.0, 'penalty': 'l1'}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 1.0, 'penalty': 'l2'}\n",
      "nan nan {'C': 0.1, 'penalty': 'l1'}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.1, 'penalty': 'l2'}\n",
      "nan nan {'C': 0.01, 'penalty': 'l1'}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.01, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "#Now cross validating our results(the score achieved in train score with validation score)\n",
    "val_scores = logreg_cv.cv_results_[\"mean_test_score\"]\n",
    "train_scores = logreg_cv.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in logreg_cv.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can see the score is almost same for train and validation set in almost all parameters and there is no sign of underfitting and overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/lr-clf.joblib']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(logreg_cv.best_estimator_, 'models/lr-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Random Forest Model using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 153.51374888420105 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing Grid search library functions\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Loading RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "# 4 x 3 x 2 = 24 combinations in the grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 200, 1000],\n",
    "    'max_depth': [3, 5, 15],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'random_state': [7]\n",
    "}\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "grid_search.fit(Xtrain, ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=15, min_samples_split=5, n_estimators=10,\n",
       "                       random_state=7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best estimators\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best hyperparameters prove to be `n_estimators=10`, `max_depth=15` and `min_sample_split=5`. They achieve the F-score of **0.299**,which is not a good score but still slightly better than basline model which was **0.295**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2992230800456154"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best result of this algoritham\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code will generate the rank of best performing model in this algoritham.\n",
    "grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will store the best model in each split for future use.\n",
    "rf_split_test_scores = []\n",
    "for x in range(5):\n",
    "    # extract f-score of the best model (index=18) from each of the 5 splits\n",
    "    val = grid_search.cv_results_[f\"split{x}_test_score\"][16]\n",
    "    rf_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2992230800456154 0.3339597346850811 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 7}\n",
      "0.2970797295101349 0.31688327264368665 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 7}\n",
      "0.2967758932271479 0.31724248463870597 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 7}\n",
      "0.29667088374061673 0.31481074470674064 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.29614222618672814 0.30723721338988996 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 10, 'random_state': 7}\n",
      "0.2959162656608636 0.30066156005350353 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 7}\n",
      "0.29586841738895864 0.30190980346264534 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 7}\n",
      "0.2957090978489833 0.3006620231987386 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.29552808431814326 0.2956835143992186 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 10, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 10, 'random_state': 7}\n"
     ]
    }
   ],
   "source": [
    "#This code will generate all results of random forest with all parameters for further analysis\n",
    "val_scores = grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we can see above that F-score almost remain the same for max_depth 3 to 15, but the best score was at max_depth 15 and n_estimators 10. However, at higher values of this hyperparameter, we notice some evidence of overfitting: the performance on training parts is slightly better than on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night: 0.09488051450906831\n",
      "Wet: 0.08907322821499625\n",
      "Urban: 0.06617088307234317\n",
      "Raining: 0.06515094151981839\n",
      "Daylight: 0.061746347207961835\n",
      "Sunday: 0.043626772599536255\n",
      "Visible: 0.03938980842474845\n",
      "No High Winds: 0.039206221054966746\n",
      "Saturday: 0.035450856223896945\n",
      "Other: 0.03511784436187076\n",
      "Tuesday: 0.034277446506207954\n",
      "Thursday: 0.03420119187786092\n",
      "Monday: 0.0300314351296972\n",
      "Wednesday: 0.028331745216058062\n",
      "October: 0.02616762575077648\n",
      "September: 0.02478768409291058\n",
      "March: 0.023817534149787274\n",
      "November: 0.023166718604556238\n",
      "December: 0.02194169088622779\n",
      "August: 0.02028114475258125\n",
      "January: 0.01833162017016314\n",
      "February: 0.01804118415444334\n",
      "July: 0.017917331771007895\n",
      "Frost: 0.01714938233604834\n",
      "June: 0.017066501816935625\n",
      "Darkness with lights unlit: 0.0168185162665959\n",
      "May: 0.01587744234803188\n",
      "Darkness with no lighting: 0.013716845438261061\n",
      "Fog or mist: 0.012964633082942998\n",
      "Snowing: 0.007721045575954828\n",
      "Flood: 0.00468953379225679\n",
      "Snow: 0.002888329091487357\n",
      "Number_of_Casualties: 0.0\n"
     ]
    }
   ],
   "source": [
    "# put them into a separate variable for convenience\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "# the order of the features in `feature_importances` is the same as in the Xtrain dataframe,\n",
    "# so we can \"zip\" the two and print in the descending order:\n",
    "\n",
    "for k, v in sorted(zip(feature_importances, Xtrain.columns), reverse=True):\n",
    "    print(f\"{v}: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The important feature for this model are area(Urban) Road condition(wet) and Time(Night) and least important features are Number_of_Casualties, Months(jan-december)etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/rf-clf.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(grid_search.best_estimator_, 'models/rf-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Random Forest Model with Over Sampling Using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**\n",
    "\n",
    "As our data is highly imbalanced so this may be the reason the models are not performing well so we will try to balance the data with SMOTE technique.\n",
    "\n",
    "Classic Oversampling-In this sampling the data which is less is duplicated increasing the total number of data and does not give any new information, and it is very Time consuming if the data is too large. \n",
    "\n",
    "SMOTE - Synthetic Minority Oversampling Technique\n",
    "\n",
    "This technique is used for oversampling the data but different from classic oversampling technique.This uses K-nearest neighbour algoritham.In this technique the dataset of minority class will be made equal to majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For oversampling of Target variable we are using smote\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving all values equal to majority class to all three categories in our target variable\n",
    "df = SMOTE(random_state = 3, sampling_strategy= {\"Fatal\":14020,\"Serious\":14020, \"Slight\":14020})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMOTE(random_state=3,\n",
       "      sampling_strategy={'Fatal': 14020, 'Serious': 14020, 'Slight': 14020})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the results\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the data and creating \"Xtrain_o\" and \"ytrain_o\"\n",
    "Xtrain_o, ytrain_o = df.fit_resample(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 405.24688720703125 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "#importing RandomForestClassifier library function\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Loading RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "# 4 x 3 x 2 = 24 combinations in the grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 100, 200, 1000],\n",
    "    'max_depth': [3, 5, 15],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'random_state': [7]\n",
    "}\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "grid_search_o = GridSearchCV(rf, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "grid_search_o.fit(Xtrain_o, ytrain_o)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=15, min_samples_split=5, n_estimators=200,\n",
       "                       random_state=7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displayibg the best estimator of the model\n",
    "grid_search_o.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7367259859307174"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best score of the model\n",
    "grid_search_o.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 73.67 % which is much better than baseline model with best estimators RandomForestClassifier(max_depth=15, min_samples_split=5, n_estimators=200,\n",
    "                       random_state=7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code will generate the rank of best performing model in this algoritham.\n",
    "grid_search_o.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will store the best model in each split for future use.\n",
    "rfo_split_test_scores = []\n",
    "for x in range(5):\n",
    "    # extract f-score of the best model (index=18) from each of the 5 splits\n",
    "    val = grid_search_o.cv_results_[f\"split{x}_test_score\"][18]\n",
    "    rfo_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7367259859307174 0.770327638649544 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 7}\n",
      "0.735259718094021 0.7693189528874306 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.7339445340637681 0.7663228871030804 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 7}\n",
      "0.7334101898934541 0.7682485281709168 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 7}\n",
      "0.7333812224908314 0.7654321312903585 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.7300015645884234 0.7637901912762886 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 7}\n",
      "0.713768519620236 0.7484696598395021 {'max_depth': 15, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 7}\n",
      "0.7093902332422473 0.7443415881006699 {'max_depth': 15, 'min_samples_split': 10, 'n_estimators': 10, 'random_state': 7}\n",
      "0.5151999325098577 0.5226534805318518 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 7}\n",
      "0.5135664853850906 0.5212484042235237 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 7}\n",
      "0.5105874212275513 0.5173669477837214 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 7}\n",
      "0.5098357784942004 0.5194063835167586 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.5091370787308112 0.5166579704309938 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 7}\n",
      "0.5085595440605644 0.5188290505952574 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.49385261210818043 0.503218245171656 {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 10, 'random_state': 7}\n",
      "0.4930448307514471 0.5019197601788182 {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 7}\n",
      "0.4624439540517895 0.46651317241991075 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 200, 'random_state': 7}\n",
      "0.4624439540517895 0.4664846170442972 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 200, 'random_state': 7}\n",
      "0.46151751932227053 0.4623014666601429 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 100, 'random_state': 7}\n",
      "0.4614986384635829 0.4622969219575593 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100, 'random_state': 7}\n",
      "0.45403936469977435 0.45693304921531547 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.4540214534697113 0.45697949370611035 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 1000, 'random_state': 7}\n",
      "0.4510372319697389 0.45406595210959655 {'max_depth': 3, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 7}\n",
      "0.4510372319697389 0.45406595210959655 {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 10, 'random_state': 7}\n"
     ]
    }
   ],
   "source": [
    "#This code will generate all results of random forest with all parameters for further analysis\n",
    "val_scores = grid_search_o.cv_results_[\"mean_test_score\"]\n",
    "train_scores = grid_search_o.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in grid_search_o.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Random Forest Model with Over Sampling varies a lot across the runs,the accuracy is 0.4510 to 0.7367 across runs.In particular, we notice that the best performance is achieved with max depth and min samples split. In this model we notice overfitting as the performance on training parts is slightly better than as on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban: 0.14696626244109207\n",
      "Night: 0.07142242167277107\n",
      "Wet: 0.06442997193173287\n",
      "Daylight: 0.05046764814749343\n",
      "Thursday: 0.04442778718968812\n",
      "Raining: 0.043866496254905955\n",
      "July: 0.043860035767904024\n",
      "Monday: 0.03836346923611518\n",
      "January: 0.035743783169415046\n",
      "November: 0.03560680263952786\n",
      "Sunday: 0.03513129918409347\n",
      "February: 0.03325933680951725\n",
      "October: 0.03244034935755542\n",
      "Visible: 0.029191425081565615\n",
      "Saturday: 0.029101616861384554\n",
      "Wednesday: 0.028984731897671583\n",
      "Tuesday: 0.0287340074269549\n",
      "Darkness with no lighting: 0.02685082713164018\n",
      "September: 0.025459311964189418\n",
      "June: 0.025135920515018166\n",
      "December: 0.021606688053839567\n",
      "May: 0.021043593452267244\n",
      "March: 0.020507605239521824\n",
      "Other: 0.017181832558748764\n",
      "No High Winds: 0.01670352479058616\n",
      "August: 0.01434938140543167\n",
      "Fog or mist: 0.007680002858318951\n",
      "Frost: 0.004521879754008683\n",
      "Darkness with lights unlit: 0.004122400302532875\n",
      "Snowing: 0.0018002885498671448\n",
      "Flood: 0.000558680890390433\n",
      "Snow: 0.0004806174642506614\n",
      "Number_of_Casualties: 0.0\n"
     ]
    }
   ],
   "source": [
    "# put them into a separate variable for convenience\n",
    "feature_importances = grid_search_o.best_estimator_.feature_importances_\n",
    "\n",
    "# the order of the features in `feature_importances` is the same as in the Xtrain dataframe,\n",
    "# so we can \"zip\" the two and print in the descending order:\n",
    "\n",
    "for k, v in sorted(zip(feature_importances, Xtrain.columns), reverse=True):\n",
    "    print(f\"{v}: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features according to this model are Urban(Urban or Rural), Night(Time) and wet(Road condition).\n",
    "\n",
    "Least Important features according to this model are Number_of_Casualties, snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/rfo-clf.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(grid_search_o.best_estimator_, 'models/rfo-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Linear Support Vector Machines with Grid search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 25.778316974639893 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Importing Linear svc\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm = LinearSVC()\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "# 5 combinations in the grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 3, 5],\n",
    "    'max_iter': [5000],\n",
    "    'random_state': [7]\n",
    "}    \n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "l_grid_search = GridSearchCV(lsvm, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "l_grid_search.fit(Xtrain, ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.001, max_iter=5000, random_state=7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best Estimators of this model\n",
    "l_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29552808431814326"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best score of this model\n",
    "l_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 29.55 %  which is slight better than baseline model with best estimators SLinearSVC(C=0.001, max_iter=5000, random_state=7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "l_grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will store the best model in each split for future use.\n",
    "ls_split_test_scores = []\n",
    "for x in range(5):\n",
    "    # extract f-score of the best model (index=0) from each of the 5 splits\n",
    "    val = l_grid_search.cv_results_[f\"split{x}_test_score\"][0]\n",
    "    ls_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29552808431814326 0.29552808265629865 {'C': 5, 'max_iter': 5000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 3, 'max_iter': 5000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 1, 'max_iter': 5000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.1, 'max_iter': 5000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.01, 'max_iter': 5000, 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.001, 'max_iter': 5000, 'random_state': 7}\n"
     ]
    }
   ],
   "source": [
    "#This code will generate all results of random forest with all parameters for further analysis\n",
    "val_scores = l_grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = l_grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in l_grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Linear Support Vector Machines did not varies a lot across the runs,the accuracy is 0.2955 across all runs.In particular, we notice that same performance is achieved with any type of parameters\". In this model we did not notice any type of overfitting as the performance on training parts is almost same as on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/ls-clf.joblib']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(l_grid_search.best_estimator_, 'models/ls-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Linear Support Vector Machines Grid Search with over sampling\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**\n",
    "\n",
    "HERE WE ARE USING THE SAME DATA WHICH IS USED ABOVE FOR RANDOM FOREST USING SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 64.36532211303711 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing linear SVC FROM sklearn.svm\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvm = LinearSVC()\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "# 5 combinations in the grid\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 3, 5],\n",
    "    'max_iter': [5000],\n",
    "    'random_state': [7]\n",
    "}    \n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "lo_grid_search = GridSearchCV(lsvm, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "lo_grid_search.fit(Xtrain_o, ytrain_o)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, max_iter=5000, random_state=7)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best estimator\n",
    "lo_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43842011162453876"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best score of the model\n",
    "lo_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 43.84 % which is far better than baseline model with best estimators LinearSVC(C=1, max_iter=5000, random_state=7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "lo_grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code will store the best model in each split for future use.\n",
    "lo_split_test_scores = []\n",
    "for x in range(5):\n",
    "    # extract f-score of the best model (index=3) from each of the 5 splits\n",
    "    val = lo_grid_search.cv_results_[f\"split{x}_test_score\"][3]\n",
    "    lo_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43842011162453876 0.4398627254595381 {'C': 1, 'max_iter': 5000, 'random_state': 7}\n",
      "0.4383312660841945 0.43991166602005605 {'C': 3, 'max_iter': 5000, 'random_state': 7}\n",
      "0.4383306137748419 0.4399129820985907 {'C': 5, 'max_iter': 5000, 'random_state': 7}\n",
      "0.43786364957523827 0.43970381380730483 {'C': 0.1, 'max_iter': 5000, 'random_state': 7}\n",
      "0.43727179906857866 0.43927045713001533 {'C': 0.01, 'max_iter': 5000, 'random_state': 7}\n",
      "0.4337141052199618 0.43626207441202514 {'C': 0.001, 'max_iter': 5000, 'random_state': 7}\n"
     ]
    }
   ],
   "source": [
    "#Recording the scores achieved by all the models in the search grid:\n",
    "val_scores = lo_grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = lo_grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in lo_grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Linear Support Vector Machines with over sampling did not varies a lot across the runs, between 0.4337 and 0.4384 In particular, we notice that better performance is achieved with lower values of \"C\". In this model we did not notice any type of overfitting as the performance on training parts is almost same as on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/lo-clf.joblib']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(lo_grid_search.best_estimator_, 'models/lo-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Radial Basis Function using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 2288.219531059265 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing SVM from sklearn.svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "# 6 x 3 = 18 combinations in the grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': [\"scale\", \"auto\", 0.1],\n",
    "    'kernel': [\"rbf\"],\n",
    "    'random_state': [7]\n",
    "}\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "rb_grid_search = GridSearchCV(svm, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "rb_grid_search.fit(Xtrain, ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, random_state=7)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best estimator\n",
    "rb_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3111366672685156"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best score\n",
    "rb_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 31.11% still better than baseline model with best estimators SVC(C=100, random_state=7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "rb_grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recording the results of the best model in each split, for future reference.\n",
    "rb_split_test_scores = []\n",
    "for x in range(5):\n",
    "# extract f-score of the best model (index=12) from each of the 5 splits \n",
    "    val = rb_grid_search.cv_results_[f\"split{x}_test_score\"][12] \n",
    "    rb_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3111366672685156 0.4439131000048964 {'C': 100, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.3020562784976847 0.38687286137706417 {'C': 100, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 7}\n",
      "0.2962515708956872 0.3366629204208859 {'C': 10, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 10, 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 1, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 1, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 1, 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.1, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.1, 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.01, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.01, 'gamma': 'scale', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.01, 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29546938919709803 0.297304512283211 {'C': 10, 'gamma': 0.1, 'kernel': 'rbf', 'random_state': 7}\n",
      "0.29546938919709803 0.29629804639139334 {'C': 100, 'gamma': 'auto', 'kernel': 'rbf', 'random_state': 7}\n"
     ]
    }
   ],
   "source": [
    "#Let's review the scores achieved by all the models in the search grid:\n",
    "val_scores = rb_grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = rb_grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in rb_grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Radial basis function varies a lot across the runs, between 0.2955 and 0.3111 In particular, we notice that better performance is achieved with higher values of \"C\". However, at higher values of this hyperparameter, we notice some evidence of overfitting: the performance on training parts is considerably better than on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/rb-clf.joblib']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(rb_grid_search.best_estimator_, 'models/rb-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Polynomial SVM using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 312.99900698661804 seconds\n"
     ]
    }
   ],
   "source": [
    "#importing GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Importing SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_poly = SVC(kernel=\"poly\", gamma=\"scale\", random_state=7)\n",
    "\n",
    "# specify the hyperparameters and their values\n",
    "# 6 x 3 = 18 combinations in the grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'degree': [2],\n",
    "}\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "poly_grid_search = GridSearchCV(svm_poly, param_grid, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "poly_grid_search.fit(Xtrain, ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, degree=2, kernel='poly', random_state=7)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best estimators\n",
    "poly_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29656597575743315"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best score\n",
    "poly_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 29.65% still better than baseline model with best estimators SVC(C=100, degree=2, kernel='poly', random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "poly_grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recording the results of the best model in each split, for future reference.\n",
    "poly_split_test_scores = []\n",
    "for x in range(5):\n",
    "# extract f-score of the best model (index=4) from each of the 5 splits \n",
    "    val = poly_grid_search.cv_results_[f\"split{x}_test_score\"][4] \n",
    "    poly_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29656597575743315 0.32787033166313506 {'C': 100, 'degree': 2}\n",
      "0.29582384353225877 0.29869371580605825 {'C': 10, 'degree': 2}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 1, 'degree': 2}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.1, 'degree': 2}\n",
      "0.29552808431814326 0.29552808265629865 {'C': 0.01, 'degree': 2}\n"
     ]
    }
   ],
   "source": [
    "#Let's review the scores achieved by all the models in the search grid:\n",
    "val_scores = poly_grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = poly_grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in poly_grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of polynomial SVM varies a lot across the runs, between 0.2955 and 0.2965 In particular, we notice that better performance is achieved with higher values of \"C\". However, at higher values of this hyperparameter, we notice some evidence of overfitting: the performance on training parts is considerably better than on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/poly-clf.joblib']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(poly_grid_search.best_estimator_, 'models/poly-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 KNN Using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1068.671002149582 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing GridSearchCV from sklearn.model_selection library\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "parameters = {\"n_neighbors\": range(1, 50)}\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "k_grid_search = GridSearchCV(knn, parameters, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "k_grid_search.fit(Xtrain, ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best estimator \n",
    "k_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3390360693248141"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying best score\n",
    "k_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 33.90% still better than baseline model with best estimators KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "k_grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's record the results of the best model in each split, for future reference.\n",
    "k_split_test_scores = []\n",
    "for x in range(5):\n",
    "# extract f-score of the best model (index=0) from each of the 5 splits \n",
    "    val = k_grid_search.cv_results_[f\"split{x}_test_score\"][0] \n",
    "    k_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3390360693248141 0.47198606668372733 {'n_neighbors': 1}\n",
      "0.33234622628213684 0.40663371448377267 {'n_neighbors': 4}\n",
      "0.32812352408760914 0.460962649423266 {'n_neighbors': 2}\n",
      "0.32747717084545 0.4283224824835526 {'n_neighbors': 3}\n",
      "0.3269122261394226 0.374188333217655 {'n_neighbors': 6}\n",
      "0.32308413471157615 0.3501385524726267 {'n_neighbors': 8}\n",
      "0.3230053382047545 0.36966871588312344 {'n_neighbors': 5}\n",
      "0.31607283085467464 0.3370740342732999 {'n_neighbors': 7}\n",
      "0.31517813311661724 0.33200400050433265 {'n_neighbors': 10}\n",
      "0.3104265091623169 0.3233718622016645 {'n_neighbors': 9}\n",
      "0.3097407275428552 0.31951549935352974 {'n_neighbors': 12}\n",
      "0.30632866030695666 0.314754108388568 {'n_neighbors': 11}\n",
      "0.3050004611928568 0.31240112895557465 {'n_neighbors': 14}\n",
      "0.3035352552774536 0.309083646535388 {'n_neighbors': 13}\n",
      "0.30089078739467073 0.30606973414662997 {'n_neighbors': 16}\n",
      "0.30083609693874347 0.30489136553554835 {'n_neighbors': 15}\n",
      "0.29863513617832804 0.3033155433133065 {'n_neighbors': 18}\n",
      "0.2985630808789 0.3034001261942183 {'n_neighbors': 17}\n",
      "0.2973576671968813 0.30127503467043015 {'n_neighbors': 20}\n",
      "0.2972724159264584 0.2996508540487335 {'n_neighbors': 23}\n",
      "0.29698856209668417 0.3000266949480671 {'n_neighbors': 24}\n",
      "0.29686668810489436 0.2996132804317592 {'n_neighbors': 22}\n",
      "0.29665502985104897 0.2997930007533031 {'n_neighbors': 21}\n",
      "0.2966463307877613 0.29948747278407006 {'n_neighbors': 26}\n",
      "0.29618903836130234 0.29667344630630793 {'n_neighbors': 30}\n",
      "0.29595082129852673 0.2971749087363401 {'n_neighbors': 29}\n",
      "0.29587945822713674 0.29822630350612817 {'n_neighbors': 27}\n",
      "0.29582730811751845 0.29636229300187067 {'n_neighbors': 31}\n",
      "0.29573497578267216 0.2980590156789777 {'n_neighbors': 25}\n",
      "0.29571118939554275 0.29562882487104436 {'n_neighbors': 41}\n",
      "0.29571118939554275 0.29562882487104436 {'n_neighbors': 40}\n",
      "0.29569974806334726 0.29562295699347707 {'n_neighbors': 38}\n",
      "0.2956863899334574 0.29785547956591196 {'n_neighbors': 28}\n",
      "0.29566729537839176 0.2997192543164364 {'n_neighbors': 19}\n",
      "0.29566452630808643 0.296399744641272 {'n_neighbors': 36}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 49}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 48}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 47}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 46}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 45}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 44}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 43}\n",
      "0.29552808431814326 0.29552808265629865 {'n_neighbors': 42}\n",
      "0.29550460761896724 0.29562882487104436 {'n_neighbors': 39}\n",
      "0.29550460761896724 0.29562882487104436 {'n_neighbors': 37}\n",
      "0.2954693858637064 0.29640561251883935 {'n_neighbors': 35}\n",
      "0.2954693858637064 0.29635144176533484 {'n_neighbors': 34}\n",
      "0.29545764194476065 0.2964032889045471 {'n_neighbors': 33}\n",
      "0.29545764194476065 0.29635144176533484 {'n_neighbors': 32}\n"
     ]
    }
   ],
   "source": [
    "#Let's review the scores achieved by all the models in the search grid:\n",
    "val_scores = k_grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = k_grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in k_grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of knn varies a lot across the runs, between 0.29 and 0.3390 In particular, we notice that better performance is achieved with lower values of n_neighbors. However, at higher values of this hyperparameter, we notice some evidence of overfitting: the performance on training parts is considerably better than on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/knn-clf.joblib']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(k_grid_search.best_estimator_, 'models/knn-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Descision tree Using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 101.18449211120605 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "descision = DecisionTreeClassifier()\n",
    "\n",
    "#specifying the hyper-parametrs and their values\n",
    "param = {\n",
    "    \"criterion\":['gini','entropy'],\n",
    "    \"max_depth\":range(1,100)\n",
    "   \n",
    "}\n",
    "# we'll use 5-fold cross-validation\n",
    "d_grid_search = GridSearchCV(descision, param, cv=5,\n",
    "                           scoring='f1_macro', \n",
    "                           return_train_score=True) \n",
    "\n",
    "start = time.time()\n",
    "d_grid_search.fit(Xtrain, ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=82)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best estimators\n",
    "d_grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3352798239543235"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displying the best score of this model\n",
    "d_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 33.52% still better than baseline model with best estimators DecisionTreeClassifier(criterion='entropy', max_depth=82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "d_grid_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_split_test_scores = []\n",
    "for x in range(5):\n",
    "# extract f-score of the best model (index=180) from each of the 5 splits \n",
    "    val = d_grid_search.cv_results_[f\"split{x}_test_score\"][180] \n",
    "    d_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3352798239543235 0.516959359685351 {'criterion': 'entropy', 'max_depth': 82}\n",
      "0.33514109493829775 0.516959359685351 {'criterion': 'entropy', 'max_depth': 34}\n",
      "0.33513872421000124 0.516959359685351 {'criterion': 'entropy', 'max_depth': 78}\n",
      "0.33507541526100726 0.516959359685351 {'criterion': 'entropy', 'max_depth': 84}\n",
      "0.33493874346622543 0.516959359685351 {'criterion': 'entropy', 'max_depth': 60}\n",
      "0.3348065260290693 0.516959359685351 {'criterion': 'entropy', 'max_depth': 89}\n",
      "0.33479679428065945 0.5167076248808573 {'criterion': 'entropy', 'max_depth': 27}\n",
      "0.33371796605931386 0.5030012021695842 {'criterion': 'entropy', 'max_depth': 21}\n",
      "0.3332773403015534 0.516959359685351 {'criterion': 'gini', 'max_depth': 99}\n",
      "0.3332423766374758 0.516959359685351 {'criterion': 'entropy', 'max_depth': 66}\n",
      "0.3331689839098572 0.516959359685351 {'criterion': 'entropy', 'max_depth': 42}\n",
      "0.3331396607586707 0.516959359685351 {'criterion': 'entropy', 'max_depth': 29}\n",
      "0.3331178183320136 0.516959359685351 {'criterion': 'entropy', 'max_depth': 90}\n",
      "0.3330804097012415 0.516959359685351 {'criterion': 'gini', 'max_depth': 71}\n",
      "0.33307535356844076 0.516959359685351 {'criterion': 'entropy', 'max_depth': 64}\n",
      "0.3330732779207263 0.516959359685351 {'criterion': 'entropy', 'max_depth': 75}\n",
      "0.3330063556807561 0.516959359685351 {'criterion': 'entropy', 'max_depth': 61}\n",
      "0.3329894447476791 0.516959359685351 {'criterion': 'entropy', 'max_depth': 44}\n",
      "0.3329657979968252 0.516959359685351 {'criterion': 'entropy', 'max_depth': 96}\n",
      "0.33296482669880617 0.516959359685351 {'criterion': 'entropy', 'max_depth': 73}\n",
      "0.33295027969764335 0.516959359685351 {'criterion': 'gini', 'max_depth': 41}\n",
      "0.3329184072551744 0.516959359685351 {'criterion': 'entropy', 'max_depth': 43}\n",
      "0.3328992385247439 0.516959359685351 {'criterion': 'entropy', 'max_depth': 67}\n",
      "0.33286526261301286 0.516959359685351 {'criterion': 'gini', 'max_depth': 73}\n",
      "0.33280985679849573 0.516959359685351 {'criterion': 'gini', 'max_depth': 45}\n",
      "0.33280900394925117 0.516959359685351 {'criterion': 'entropy', 'max_depth': 69}\n",
      "0.33280843166178914 0.516959359685351 {'criterion': 'gini', 'max_depth': 96}\n",
      "0.3327929596277818 0.516959359685351 {'criterion': 'entropy', 'max_depth': 72}\n",
      "0.3327826065294032 0.516959359685351 {'criterion': 'entropy', 'max_depth': 80}\n",
      "0.3327756908737042 0.516959359685351 {'criterion': 'entropy', 'max_depth': 48}\n",
      "0.33277244829593566 0.516959359685351 {'criterion': 'gini', 'max_depth': 69}\n",
      "0.3327719357368343 0.516959359685351 {'criterion': 'entropy', 'max_depth': 40}\n",
      "0.33274983968602423 0.516959359685351 {'criterion': 'gini', 'max_depth': 50}\n",
      "0.33274630259054294 0.516959359685351 {'criterion': 'entropy', 'max_depth': 81}\n",
      "0.3327310298575778 0.5154347011163755 {'criterion': 'gini', 'max_depth': 26}\n",
      "0.3327247896250611 0.516959359685351 {'criterion': 'gini', 'max_depth': 82}\n",
      "0.33270816221348676 0.516959359685351 {'criterion': 'entropy', 'max_depth': 63}\n",
      "0.3326945046462624 0.516959359685351 {'criterion': 'gini', 'max_depth': 32}\n",
      "0.3326899772472248 0.5168587704994867 {'criterion': 'gini', 'max_depth': 29}\n",
      "0.33267464302885474 0.516959359685351 {'criterion': 'entropy', 'max_depth': 30}\n",
      "0.3326616400169562 0.516959359685351 {'criterion': 'entropy', 'max_depth': 87}\n",
      "0.3326192581444893 0.516959359685351 {'criterion': 'gini', 'max_depth': 51}\n",
      "0.3326171720576311 0.516959359685351 {'criterion': 'entropy', 'max_depth': 83}\n",
      "0.33261044505352516 0.516959359685351 {'criterion': 'entropy', 'max_depth': 65}\n",
      "0.33260202894256874 0.516959359685351 {'criterion': 'entropy', 'max_depth': 88}\n",
      "0.33259851972345433 0.516959359685351 {'criterion': 'entropy', 'max_depth': 92}\n",
      "0.3325768943137749 0.516959359685351 {'criterion': 'gini', 'max_depth': 36}\n",
      "0.3325404923089078 0.516959359685351 {'criterion': 'gini', 'max_depth': 62}\n",
      "0.33252116039143187 0.516959359685351 {'criterion': 'entropy', 'max_depth': 97}\n",
      "0.33251077710209154 0.516959359685351 {'criterion': 'gini', 'max_depth': 77}\n",
      "0.3325037823183116 0.516959359685351 {'criterion': 'gini', 'max_depth': 53}\n",
      "0.33250273604361735 0.516959359685351 {'criterion': 'gini', 'max_depth': 83}\n",
      "0.33249266121356613 0.516959359685351 {'criterion': 'gini', 'max_depth': 44}\n",
      "0.33247266040615114 0.516959359685351 {'criterion': 'gini', 'max_depth': 78}\n",
      "0.332455339441032 0.516959359685351 {'criterion': 'entropy', 'max_depth': 31}\n",
      "0.3324148105861867 0.5161052178392491 {'criterion': 'gini', 'max_depth': 27}\n",
      "0.33236579107685993 0.516959359685351 {'criterion': 'entropy', 'max_depth': 47}\n",
      "0.33236001075488525 0.516959359685351 {'criterion': 'gini', 'max_depth': 49}\n",
      "0.33231159619627976 0.516959359685351 {'criterion': 'entropy', 'max_depth': 50}\n",
      "0.3323012639593218 0.516959359685351 {'criterion': 'gini', 'max_depth': 34}\n",
      "0.3322106528940741 0.516959359685351 {'criterion': 'gini', 'max_depth': 59}\n",
      "0.3322037539675635 0.516959359685351 {'criterion': 'gini', 'max_depth': 57}\n",
      "0.3321995998249435 0.516959359685351 {'criterion': 'gini', 'max_depth': 86}\n",
      "0.3321903348819937 0.516959359685351 {'criterion': 'gini', 'max_depth': 47}\n",
      "0.3321759263518901 0.516959359685351 {'criterion': 'gini', 'max_depth': 81}\n",
      "0.33216116459345096 0.516959359685351 {'criterion': 'gini', 'max_depth': 97}\n",
      "0.3321592225106763 0.516959359685351 {'criterion': 'entropy', 'max_depth': 37}\n",
      "0.33214801331569993 0.516959359685351 {'criterion': 'gini', 'max_depth': 46}\n",
      "0.3321231943396072 0.516959359685351 {'criterion': 'entropy', 'max_depth': 99}\n",
      "0.3321211069748749 0.516959359685351 {'criterion': 'gini', 'max_depth': 38}\n",
      "0.33201257676394336 0.516959359685351 {'criterion': 'gini', 'max_depth': 72}\n",
      "0.3319888592109886 0.516959359685351 {'criterion': 'gini', 'max_depth': 60}\n",
      "0.3319691302377994 0.516959359685351 {'criterion': 'gini', 'max_depth': 91}\n",
      "0.3319214367393493 0.516959359685351 {'criterion': 'gini', 'max_depth': 64}\n",
      "0.331881489962133 0.516959359685351 {'criterion': 'gini', 'max_depth': 40}\n",
      "0.3318626459476594 0.516959359685351 {'criterion': 'gini', 'max_depth': 85}\n",
      "0.33185741514368744 0.516959359685351 {'criterion': 'gini', 'max_depth': 89}\n",
      "0.3318218952611195 0.516959359685351 {'criterion': 'gini', 'max_depth': 74}\n",
      "0.33170963939864284 0.5164930548131543 {'criterion': 'entropy', 'max_depth': 26}\n",
      "0.33169960361669293 0.516959359685351 {'criterion': 'gini', 'max_depth': 56}\n",
      "0.3315186139065166 0.516959359685351 {'criterion': 'gini', 'max_depth': 94}\n",
      "0.33148833551013535 0.516959359685351 {'criterion': 'gini', 'max_depth': 68}\n",
      "0.33142773264690345 0.516959359685351 {'criterion': 'gini', 'max_depth': 70}\n",
      "0.3312581467260706 0.516959359685351 {'criterion': 'gini', 'max_depth': 31}\n",
      "0.331237810822974 0.516959359685351 {'criterion': 'entropy', 'max_depth': 93}\n",
      "0.33118549587351376 0.516959359685351 {'criterion': 'gini', 'max_depth': 95}\n",
      "0.3311659064095668 0.516959359685351 {'criterion': 'entropy', 'max_depth': 77}\n",
      "0.33106402570265614 0.516959359685351 {'criterion': 'entropy', 'max_depth': 54}\n",
      "0.3309208271664024 0.516959359685351 {'criterion': 'gini', 'max_depth': 76}\n",
      "0.3308965731332176 0.516959359685351 {'criterion': 'entropy', 'max_depth': 59}\n",
      "0.3308949062486537 0.516959359685351 {'criterion': 'gini', 'max_depth': 33}\n",
      "0.3308646129241218 0.516959359685351 {'criterion': 'gini', 'max_depth': 39}\n",
      "0.3308588457957241 0.516959359685351 {'criterion': 'gini', 'max_depth': 79}\n",
      "0.33085431426934936 0.5154254494048052 {'criterion': 'entropy', 'max_depth': 25}\n",
      "0.3308233066128282 0.516959359685351 {'criterion': 'entropy', 'max_depth': 39}\n",
      "0.33082046233843154 0.516959359685351 {'criterion': 'entropy', 'max_depth': 62}\n",
      "0.3308021474202822 0.516959359685351 {'criterion': 'entropy', 'max_depth': 33}\n",
      "0.3307774860710248 0.516959359685351 {'criterion': 'entropy', 'max_depth': 46}\n",
      "0.3307568490795905 0.516959359685351 {'criterion': 'entropy', 'max_depth': 35}\n",
      "0.33074774428252324 0.5168545663676896 {'criterion': 'entropy', 'max_depth': 28}\n",
      "0.3306834120681748 0.516959359685351 {'criterion': 'entropy', 'max_depth': 71}\n",
      "0.33066388693174426 0.516959359685351 {'criterion': 'entropy', 'max_depth': 38}\n",
      "0.33064309154811955 0.5096887187976991 {'criterion': 'entropy', 'max_depth': 22}\n",
      "0.33062737544752246 0.516959359685351 {'criterion': 'gini', 'max_depth': 90}\n",
      "0.33061201300666015 0.516959359685351 {'criterion': 'gini', 'max_depth': 88}\n",
      "0.33059618674309654 0.516959359685351 {'criterion': 'entropy', 'max_depth': 58}\n",
      "0.33059597771519955 0.516959359685351 {'criterion': 'entropy', 'max_depth': 94}\n",
      "0.33056870976012526 0.516959359685351 {'criterion': 'gini', 'max_depth': 37}\n",
      "0.3305589490401091 0.516959359685351 {'criterion': 'gini', 'max_depth': 80}\n",
      "0.330545404155957 0.516959359685351 {'criterion': 'entropy', 'max_depth': 57}\n",
      "0.3305396623884934 0.516959359685351 {'criterion': 'gini', 'max_depth': 93}\n",
      "0.3305287332203463 0.516959359685351 {'criterion': 'entropy', 'max_depth': 86}\n",
      "0.3304631443872301 0.516959359685351 {'criterion': 'gini', 'max_depth': 35}\n",
      "0.3304440571428027 0.516959359685351 {'criterion': 'entropy', 'max_depth': 76}\n",
      "0.330375542858859 0.516959359685351 {'criterion': 'gini', 'max_depth': 75}\n",
      "0.33035094014718447 0.516959359685351 {'criterion': 'entropy', 'max_depth': 56}\n",
      "0.3303387907311385 0.516959359685351 {'criterion': 'gini', 'max_depth': 67}\n",
      "0.33032876926807875 0.516959359685351 {'criterion': 'entropy', 'max_depth': 51}\n",
      "0.3303262378245827 0.516959359685351 {'criterion': 'entropy', 'max_depth': 55}\n",
      "0.33032323248530304 0.516959359685351 {'criterion': 'entropy', 'max_depth': 79}\n",
      "0.3303218255780904 0.516959359685351 {'criterion': 'gini', 'max_depth': 84}\n",
      "0.33027216774076545 0.516959359685351 {'criterion': 'entropy', 'max_depth': 53}\n",
      "0.3302582532337925 0.516959359685351 {'criterion': 'gini', 'max_depth': 87}\n",
      "0.33022114900903093 0.516959359685351 {'criterion': 'entropy', 'max_depth': 98}\n",
      "0.3302039954928147 0.516959359685351 {'criterion': 'gini', 'max_depth': 48}\n",
      "0.3301870891505011 0.516959359685351 {'criterion': 'entropy', 'max_depth': 95}\n",
      "0.3301649051291095 0.516959359685351 {'criterion': 'entropy', 'max_depth': 91}\n",
      "0.33015037780155615 0.5125114903019596 {'criterion': 'entropy', 'max_depth': 23}\n",
      "0.3301417410995259 0.516959359685351 {'criterion': 'gini', 'max_depth': 54}\n",
      "0.33011387367459927 0.516959359685351 {'criterion': 'entropy', 'max_depth': 52}\n",
      "0.3301098868863403 0.516959359685351 {'criterion': 'gini', 'max_depth': 43}\n",
      "0.33010171638162245 0.516959359685351 {'criterion': 'entropy', 'max_depth': 36}\n",
      "0.3300969934822901 0.516959359685351 {'criterion': 'entropy', 'max_depth': 49}\n",
      "0.3300821505107098 0.516959359685351 {'criterion': 'gini', 'max_depth': 92}\n",
      "0.3300737261664916 0.516959359685351 {'criterion': 'gini', 'max_depth': 65}\n",
      "0.3300721229874847 0.516959359685351 {'criterion': 'entropy', 'max_depth': 41}\n",
      "0.33006501609670197 0.5168071552689226 {'criterion': 'gini', 'max_depth': 28}\n",
      "0.3300483032229943 0.516959359685351 {'criterion': 'gini', 'max_depth': 61}\n",
      "0.3300386568156139 0.516959359685351 {'criterion': 'gini', 'max_depth': 98}\n",
      "0.3299518275055664 0.516959359685351 {'criterion': 'gini', 'max_depth': 58}\n",
      "0.32994936206354236 0.516959359685351 {'criterion': 'gini', 'max_depth': 66}\n",
      "0.32991207399292777 0.516959359685351 {'criterion': 'gini', 'max_depth': 42}\n",
      "0.32984589825635624 0.516959359685351 {'criterion': 'entropy', 'max_depth': 85}\n",
      "0.32980915412469775 0.516959359685351 {'criterion': 'entropy', 'max_depth': 70}\n",
      "0.32979966369398755 0.5115015109556111 {'criterion': 'gini', 'max_depth': 24}\n",
      "0.32973139136648977 0.5169339559278507 {'criterion': 'gini', 'max_depth': 30}\n",
      "0.3296875450495868 0.5139895928062047 {'criterion': 'gini', 'max_depth': 25}\n",
      "0.32962417693455653 0.516959359685351 {'criterion': 'gini', 'max_depth': 52}\n",
      "0.3296028436704881 0.516959359685351 {'criterion': 'gini', 'max_depth': 55}\n",
      "0.32959196367401566 0.516959359685351 {'criterion': 'gini', 'max_depth': 63}\n",
      "0.32952830551160767 0.5056813070838594 {'criterion': 'gini', 'max_depth': 22}\n",
      "0.3286252916407437 0.5026189222537443 {'criterion': 'gini', 'max_depth': 21}\n",
      "0.3285169626577426 0.47252266679263677 {'criterion': 'entropy', 'max_depth': 17}\n",
      "0.32851133176732983 0.516959359685351 {'criterion': 'entropy', 'max_depth': 45}\n",
      "0.32848054170099494 0.4991556739597393 {'criterion': 'entropy', 'max_depth': 20}\n",
      "0.32838516802669393 0.516959359685351 {'criterion': 'entropy', 'max_depth': 68}\n",
      "0.328298274435649 0.5085569482803537 {'criterion': 'gini', 'max_depth': 23}\n",
      "0.3279882939154989 0.516959359685351 {'criterion': 'entropy', 'max_depth': 74}\n",
      "0.3279029924754481 0.49132747917452785 {'criterion': 'entropy', 'max_depth': 19}\n",
      "0.32776702214081677 0.5140528737294275 {'criterion': 'entropy', 'max_depth': 24}\n",
      "0.3276551751455335 0.516959359685351 {'criterion': 'entropy', 'max_depth': 32}\n",
      "0.3273627801190579 0.46247870782870065 {'criterion': 'entropy', 'max_depth': 16}\n",
      "0.3265902614076185 0.49730598925978986 {'criterion': 'gini', 'max_depth': 20}\n",
      "0.3254769923470084 0.4909470080290818 {'criterion': 'gini', 'max_depth': 19}\n",
      "0.3249341237619453 0.46877237278154726 {'criterion': 'gini', 'max_depth': 17}\n",
      "0.3238830340671647 0.48347692767596406 {'criterion': 'entropy', 'max_depth': 18}\n",
      "0.32284916470515346 0.47993605390951116 {'criterion': 'gini', 'max_depth': 18}\n",
      "0.3227224150853938 0.45806251434481815 {'criterion': 'gini', 'max_depth': 16}\n",
      "0.3218227619577386 0.44438788652363054 {'criterion': 'gini', 'max_depth': 15}\n",
      "0.31856441749843223 0.4288730119324845 {'criterion': 'gini', 'max_depth': 14}\n",
      "0.31842242143069543 0.4482920751935554 {'criterion': 'entropy', 'max_depth': 15}\n",
      "0.3173049363870869 0.43301183088845774 {'criterion': 'entropy', 'max_depth': 14}\n",
      "0.31667660574952383 0.4011926638621226 {'criterion': 'entropy', 'max_depth': 12}\n",
      "0.3140837176299708 0.38628342767035617 {'criterion': 'entropy', 'max_depth': 11}\n",
      "0.312559050099089 0.417047273183717 {'criterion': 'entropy', 'max_depth': 13}\n",
      "0.3117391797807523 0.41040825262192177 {'criterion': 'gini', 'max_depth': 13}\n",
      "0.31016274649410885 0.3710774988291396 {'criterion': 'entropy', 'max_depth': 10}\n",
      "0.309766771810837 0.35623817688543274 {'criterion': 'entropy', 'max_depth': 9}\n",
      "0.30702515130257046 0.3981620018944346 {'criterion': 'gini', 'max_depth': 12}\n",
      "0.3064361681466618 0.37929739270724205 {'criterion': 'gini', 'max_depth': 11}\n",
      "0.3053292772897157 0.3590382620117475 {'criterion': 'gini', 'max_depth': 10}\n",
      "0.30372211369970425 0.32240260871525345 {'criterion': 'entropy', 'max_depth': 7}\n",
      "0.3030625161632403 0.34301868886948317 {'criterion': 'gini', 'max_depth': 9}\n",
      "0.3025695211650223 0.3370903041511041 {'criterion': 'entropy', 'max_depth': 8}\n",
      "0.30212820767896653 0.3303917727406151 {'criterion': 'gini', 'max_depth': 8}\n",
      "0.30078697559165574 0.3183856338251293 {'criterion': 'gini', 'max_depth': 7}\n",
      "0.3001747105281939 0.31212283139841 {'criterion': 'gini', 'max_depth': 6}\n",
      "0.29980288922536713 0.3132558318680194 {'criterion': 'entropy', 'max_depth': 6}\n",
      "0.2996909525677463 0.30592142886642576 {'criterion': 'gini', 'max_depth': 5}\n",
      "0.2983537948194191 0.3025438479194001 {'criterion': 'entropy', 'max_depth': 4}\n",
      "0.29680655836134384 0.30239398978319754 {'criterion': 'gini', 'max_depth': 4}\n",
      "0.2957873982612836 0.3058717560818246 {'criterion': 'entropy', 'max_depth': 5}\n",
      "0.29552808431814326 0.29552808265629865 {'criterion': 'gini', 'max_depth': 2}\n",
      "0.29552808431814326 0.29552808265629865 {'criterion': 'gini', 'max_depth': 1}\n",
      "0.29552808431814326 0.29552808265629865 {'criterion': 'entropy', 'max_depth': 2}\n",
      "0.29552808431814326 0.29552808265629865 {'criterion': 'entropy', 'max_depth': 1}\n",
      "0.29550461057401944 0.29846663527403605 {'criterion': 'gini', 'max_depth': 3}\n",
      "0.29550461057401944 0.29846663527403605 {'criterion': 'entropy', 'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "#Let's review the scores achieved by all the models in the search grid:\n",
    "val_scores = d_grid_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = d_grid_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in d_grid_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Decision tree classifiers varies a lot across the runs, between 0.29 and 0.33. In particular, we notice that better performance is achieved with greater values of max_depth. However, at higher values of this hyperparameter, we notice some evidence of overfitting: the performance on training parts is considerably better than on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wet: 0.11493820290649132\n",
      "Night: 0.07285132708165853\n",
      "Raining: 0.07078758712826407\n",
      "Tuesday: 0.04822930787564663\n",
      "Thursday: 0.04245949135093552\n",
      "Wednesday: 0.036817181887156485\n",
      "No High Winds: 0.03541194844623431\n",
      "Visible: 0.034751714541261754\n",
      "Daylight: 0.03439751872718033\n",
      "Monday: 0.03426193513412502\n",
      "January: 0.033270638910824483\n",
      "September: 0.031291934033193816\n",
      "October: 0.03127037168440578\n",
      "August: 0.03091610630308548\n",
      "March: 0.030275356090448895\n",
      "November: 0.029270884727498572\n",
      "Other: 0.029078130091947044\n",
      "Sunday: 0.027194016421734253\n",
      "June: 0.02650202166387926\n",
      "December: 0.024995272969447486\n",
      "Saturday: 0.024679369260915112\n",
      "Urban: 0.023821041855641416\n",
      "February: 0.023687788569838174\n",
      "July: 0.020559730770109197\n",
      "Frost: 0.019476105702973517\n",
      "May: 0.016548459080408626\n",
      "Darkness with lights unlit: 0.015311740779336296\n",
      "Darkness with no lighting: 0.012430924987446182\n",
      "Fog or mist: 0.011285265168420204\n",
      "Snowing: 0.007493082560717993\n",
      "Flood: 0.0038977040470239044\n",
      "Snow: 0.0018378392417501975\n",
      "Number_of_Casualties: 0.0\n"
     ]
    }
   ],
   "source": [
    "# put them into a separate variable for convenience\n",
    "feature_importances = d_grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "# the order of the features in `feature_importances` is the same as in the Xtrain dataframe,\n",
    "# so we can \"zip\" the two and print in the descending order:\n",
    "\n",
    "for k, v in sorted(zip(feature_importances, Xtrain.columns), reverse=True):\n",
    "    print(f\"{v}: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 important features of this model are wet(Road condition), Night(Time), Raining(weather)\n",
    "Least important features are Flood(Road condition), snow and Number of casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/dt-clf.joblib']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(d_grid_search.best_estimator_, 'models/dt-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11 Gradient Boosting Classifier using Grid Search\n",
    "**We are using grid serach CV library function, it helps in getting best model by doing Hyperparameter tunning automatically on mentioned parameters and gives us the best parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piyushchaudhary/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 1973.9077842235565 seconds\n"
     ]
    }
   ],
   "source": [
    "#Importing grid serach from sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Importing GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "#specifying the hyper-parametrs and their values\n",
    "n_estimators = [10, 100, 1000]\n",
    "learning_rate = [0.001, 0.01, 0.1]\n",
    "subsample = [0.5, 0.7, 1.0]\n",
    "max_depth = [3, 7, 9]\n",
    "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
    "\n",
    "# we'll use 5-fold cross-validation\n",
    "gb_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=5, \n",
    "                           scoring='f1_macro',return_train_score=True)\n",
    "\n",
    "start = time.time()\n",
    "gb_search.fit(Xtrain,ytrain)\n",
    "end = time.time() - start\n",
    "print(f\"Took {end} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(max_depth=9, n_estimators=1000, subsample=0.5)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying the best estimators for this model\n",
    "gb_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3243654899710668"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best score (accuracy)\n",
    "gb_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got is 32.43% still better than baseline model with best estimators GradientBoostingClassifier(max_depth=9, n_estimators=1000, subsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the best performing model rank index\n",
    "gb_search.cv_results_[\"rank_test_score\"].tolist().index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_split_test_scores = []\n",
    "for x in range(5):\n",
    "# extract f-score of the best model (index=78) from each of the 5 splits \n",
    "    val = gb_search.cv_results_[f\"split{x}_test_score\"][78] \n",
    "    gb_split_test_scores.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3243654899710668 0.4827494093881091 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.3236765730740103 0.4825099931446825 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.31932362556197 0.47132285282499 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.3182350731974497 0.4705977975415386 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.31806526116348566 0.4863500237724249 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.3170882824827664 0.48287756825428224 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.317071767093978 0.4596957215556565 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.3155939254129743 0.4598971674405378 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.31538252432057495 0.4575316771390957 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.3136467332251443 0.43346905587057716 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.313034216264887 0.47589786963287495 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.3127557722748163 0.4535241583735078 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.31057156435043654 0.4277352836540736 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.3088960562760633 0.4554341312879388 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.30878532941214204 0.36587707067797226 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.30775593149163405 0.41816789168638435 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.30703284974341943 0.4146049697391768 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.30660736596919386 0.3478545935537799 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.30548890612722 0.4258274470067807 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.30497570793509726 0.4179402037107421 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.3034064886960977 0.3711018838282151 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.3022761730891812 0.37155739005003724 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.3022086511041875 0.3422938935046931 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.30135745220948024 0.3520973287314606 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.301175133929769 0.35780234447299775 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.301158846405359 0.35179984985809687 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.3008710318314248 0.34501724653392685 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.30012793969755175 0.359447978679912 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.2996039895919386 0.32737736513759763 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.2996039895919386 0.3271735939150622 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.29818301562584604 0.354991253684169 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.2966490225651905 0.31787077798222346 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.29663670860421953 0.31874015807287115 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.29645236648313633 0.3101939111907396 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.2963871691950394 0.3083248190348436 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.2962096698421125 0.31029691584022095 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.2960653706834939 0.3072948625706594 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.29571177838589974 0.30722703368922694 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.29570004151383783 0.3047106265737729 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.295652786858964 0.306597928159026 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.29557057303447515 0.3088465134115033 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.29552808431814326 0.30130387429774863 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29892737460315494 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.5}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.29552808431814326 0.29552808265629865 {'learning_rate': 0.001, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n",
      "0.2955163478244208 0.29627948257248804 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 1.0}\n",
      "0.2954928637000215 0.30549543481200814 {'learning_rate': 0.001, 'max_depth': 7, 'n_estimators': 1000, 'subsample': 0.7}\n",
      "0.2954811342531828 0.3014087067800545 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.7}\n",
      "0.2954811268279595 0.30393807105032844 {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.5}\n",
      "0.29544590097698303 0.2999457511294003 {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 10, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Let's review the scores achieved by all the models in the search grid:\n",
    "val_scores = gb_search.cv_results_[\"mean_test_score\"]\n",
    "train_scores = gb_search.cv_results_[\"mean_train_score\"]\n",
    "params = [str(x) for x in gb_search.cv_results_[\"params\"]]\n",
    "\n",
    "for val_score, train_score, param in sorted(zip(val_scores, train_scores, params), reverse=True):\n",
    "    print(val_score, train_score, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Gradient boosting classifiers varies a lot across the runs, between 0.29 and 0.32. In particular, we notice that better performance is achieved with greater values of `max_depth`. However, at higher values of this hyperparameter, we notice some evidence of overfitting: the performance on training parts is considerably better than on the validation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urban: 0.0990726516970581\n",
      "Night: 0.0968513899084105\n",
      "Wet: 0.08731539874549142\n",
      "Daylight: 0.07900413885341173\n",
      "Raining: 0.07585337387561703\n",
      "Tuesday: 0.03774327863543693\n",
      "Sunday: 0.03760218516312455\n",
      "Saturday: 0.03741038233052027\n",
      "Monday: 0.03711209131805721\n",
      "Thursday: 0.036023294663145355\n",
      "Wednesday: 0.0351178488544191\n",
      "Visible: 0.03502629224369002\n",
      "Darkness with no lighting: 0.03301321997882094\n",
      "No High Winds: 0.025309630662257162\n",
      "October: 0.020762944976114434\n",
      "January: 0.02061680174196086\n",
      "September: 0.01996014456820348\n",
      "August: 0.01989443401155624\n",
      "November: 0.01983506353292343\n",
      "December: 0.01884864259958577\n",
      "Other: 0.018221157180847894\n",
      "July: 0.01795333401115377\n",
      "May: 0.01708033697426926\n",
      "March: 0.016675092156103778\n",
      "June: 0.016047876640337813\n",
      "February: 0.01564827884723011\n",
      "Frost: 0.010571619609647118\n",
      "Darkness with lights unlit: 0.004380113644846227\n",
      "Fog or mist: 0.00368389816363814\n",
      "Snowing: 0.0029079388506622317\n",
      "Snow: 0.0026527254948946186\n",
      "Flood: 0.0018044200665645975\n",
      "Number_of_Casualties: 0.0\n"
     ]
    }
   ],
   "source": [
    "# put them into a separate variable for convenience\n",
    "feature_importances = gb_search.best_estimator_.feature_importances_\n",
    "\n",
    "# the order of the features in `feature_importances` is the same as in the Xtrain dataframe,\n",
    "# so we can \"zip\" the two and print in the descending order:\n",
    "\n",
    "for k, v in sorted(zip(feature_importances, Xtrain.columns), reverse=True):\n",
    "    print(f\"{v}: {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 3 important feature of this model are urban, night and wet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/gbb-clf.joblib']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This code is storing the model in \"joblib\" function for future reference, as again running the model \n",
    "#from scratch will be very time consuming, so to save time we are doing this\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "# create a folder where all trained models will be kept\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "    \n",
    "dump(gb_search.best_estimator_, 'models/gbb-clf.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing the best models\n",
    "**Now we will compare our top performing models on testset to test the accuracy of our models**\n",
    "\n",
    "#### Top Performing Models\n",
    "1. Random Forest with oversampling\n",
    "2. Linear Support Vector Machine with Oversampling\n",
    "3. KNN\n",
    "4. Descision Tree Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the models which we have saved earlier while modelling to save our time\n",
    "from joblib import load\n",
    "best_rf_o = load(\"models/rfo-clf.joblib\")\n",
    "lsvm_o = load(\"models/lo-clf.joblib\")\n",
    "knn_u = load(\"models/knn-clf.joblib\")\n",
    "des_tree_u = load(\"models/dt-clf.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Random Forest with oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with over Sampling:\n",
      "Precision: 0.3479296567603231\n",
      "Recall: 0.3592729592714116\n",
      "F score: 0.3479480381653001\n"
     ]
    }
   ],
   "source": [
    "#Importing precision, recall and fscore from sklearn.metrics library\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "yhat = best_rf_o.predict(Xtest)\n",
    "\n",
    "# micro-averaged precision, recall and f-score\n",
    "p, r, f, s = precision_recall_fscore_support(ytest, yhat, average=\"macro\")\n",
    "print(\"Random Forest with over Sampling:\")\n",
    "print(f\"Precision: {p}\")\n",
    "print(f\"Recall: {r}\")\n",
    "print(f\"F score: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see F score is very low only 34.79% of test records are predicted corretly, the score of predicting positive value (precision) is also very low only 35.92% of positive records predicted correct.\n",
    "\n",
    "Also f score is better on training data as compare to test data which suggest overfitting in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe3320368e0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing confusion matrix from sklearn.metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(best_rf_o, Xtest, ytest,\n",
    "cmap=plt.cm.Greens, normalize='true')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at above confusion matrix it can be concluded that this model predicting slight accident with 74% accuracy but not able to predict serious and fatal accident precisely. And 22% of slight accident are predicted as serious. 25% of serious and 8.3 % of fatal accident predicted correctly.\n",
    "\n",
    "Most of the accident are predicetd as slight by this model which is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Support Vector Machine with Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Support Vector Machine with Oversampling:\n",
      "Precision: 0.34694190097426664\n",
      "Recall: 0.40589609257310305\n",
      "F score: 0.28477717749660697\n"
     ]
    }
   ],
   "source": [
    "#Importing precision, recall and fscore from sklearn.metrics library\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "yhat = lsvm_o.predict(Xtest)\n",
    "\n",
    "# micro-averaged precision, recall and f-score\n",
    "p, r, f, s = precision_recall_fscore_support(ytest, yhat, average=\"macro\")\n",
    "print(\"Linear Support Vector Machine with Oversampling:\")\n",
    "print(f\"Precision: {p}\")\n",
    "print(f\"Recall: {r}\")\n",
    "print(f\"F score: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see F score is very low only 28.47% of test records are predicted corretly, the score of predicting positive value (precision) is also very low only 50.58% of positive records predicted correct.\n",
    "\n",
    "Also f score is better on training data as compare to test data which suggest overfitting in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe322dbca60>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing confusion matrix from sklearn.metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(lsvm_o, Xtest, ytest,\n",
    "cmap=plt.cm.Greens, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here in above confusion matrix we can see that any type of accident severity is not predicted accurately only 50% of fatal accident 26% of serious accident and 46% of slight accident predicted correctly.This model predicting fatal accident better than any other model which is a good thing as prediction of fatal severity is more improtant as it can have serious damages and loss as copmare to slight and serious severities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Precision: 0.34200702508581804\n",
      "Recall: 0.3426056190685826\n",
      "F score: 0.341966469187595\n"
     ]
    }
   ],
   "source": [
    "#Importing precision, recall and fscore from sklearn.metrics library\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "yhat = knn_u.predict(Xtest)\n",
    "\n",
    "# micro-averaged precision, recall and f-score\n",
    "p, r, f, s = precision_recall_fscore_support(ytest, yhat, average=\"macro\")\n",
    "print(\"KNN:\")\n",
    "print(f\"Precision: {p}\")\n",
    "print(f\"Recall: {r}\")\n",
    "print(f\"F score: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see F score is very low only 34.19% of test records are predicted corretly, the score of predicting positive value (precision) is also very low only 34.26% of positive records predicted correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe322da7e20>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAEGCAYAAAA61G1JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAuiklEQVR4nO3deXwV1fnH8c83CQgIghCWsCMg7qIgioLggqJowQVxwV0Bq0WtWrXauqNWrbZuSBW1i2tFBUHRUgFXDFhEERc2JYQtgArqT0jy/P64k3BzzTKBSe69+Lx5zYs7M2fOnLmEJ+ecmXNGZoZzzrltl5HsAjjn3PbCA6pzzkXEA6pzzkXEA6pzzkXEA6pzzkUkK9kFSAXZ2dnWvkP7ZBcjZRXa5mQXIeV9vPDzZBch9W3YXGBmzbf2cGXXMzYVhz3XVDMbuLXn2loeUIH2Hdrz9qyZyS5Gylr305pkFyHltRvSN9lFSH3/Wf7VNh2/qRgObBH2XNnbdK6t5AHVOZc+pGSXoFIeUJ1z6UFApgdU55yLRmrHUw+ozrl0IW/yO+dcJETKP+jpAdU5lz68huqccxFJ7XjqAdU5lybS4C5/ivdIOOdcHCncEiorDZT0uaSFkq4pZ39jSZMkfSRpvqRzq8rTA6pzLn0o5FJVNlIm8CBwDLAHcJqkPRKSXQx8amb7Av2BeyTVrSxfD6jOufQgIEPhlqr1Ahaa2WIz2wQ8AwxOSGNAI0kCGgLrgMLKMvU+VOdc+gjfhZotaXbc+jgzGxe33gZYFreeBxyYkMcDwEQgH2gEDDOzSmdn8YDqnEsPEmSGblQXmFnPynIrZ1viC/aOBuYChwOdgTckvWVm31WUqTf5nXPpI6I+VGI10nZx622J1UTjnQtMsJiFwBJgt8oy9YDqnEsf0d3lzwW6SuoU3Gg6lVjzPt7XwBGx06ol0A1YXFmm3uR3zqWPiB5DNbNCSZcAU4FMYLyZzZc0Ktg/FrgFeELSx8GZrzazgsry9YDqnEsPJXf5I2JmU4ApCdvGxn3OB46qTp4eUJ1z6SO1B0p5QHXOpZEUH3rqAdU5lx6qMaw0WTygOufSR2rHUw+ozrk04jVU55yLSIo/Oe8B1TmXHiJ+bKomeEB1zqUPD6jOORcR70N1zrkIhJ/4JGk8oDrn0oRQyBpq4jx8tcUDqnMubXhAdc65CAjIDHlTqtJp9WuQB9Ra8Prst/jd2NsoKi7m7IEnc+UpI8rsNzOuGnsbU3NnUn+Hejxyxe3s12VP8tas4MK7r2bV+gIylMG5x5zCxUPOAuCjRQu49P4b+b/NP5GVmcl9F99Az277JOPyIvXmh7O48dG/UFRczGkDjuPik4aX2b8w7yuuuP92Pln0BVcNv5BRQ04DIH/NKi77y22s+WYdGRKnH/Urzj9+aDIuocYN6NGXuy+6jsyMTJ547Xnufm5cmf07NWjI+N/dTbsWrWM/G/9+jH+8MQGAz578Lxt++J6i4mIKiwrpM/qkZFzC1lH4GmqypFRAlVQEfBy3aYiZLS0nXRPgdDN7KESeG82sYWSFrKaioiJ+++DNTBoznjbZLel76VAGHXg4u3foUppmau5MFuZ/xbzHppL72Udc9sBNzLjvOTIzMxlz4dXs12VPNvywkT6jT+Lw/Q5m9w5duP6xu7j2jIs5+oBDee2DGVz/2F289qd/JOsyI1FUVMT1j/yZp266l5xmzTnuqgsZ0OsQdm3XqTRNk4Y7cdMFlzJ11ltljs3MzOQP517M3p27sfHHHzj2ivPp271nmWO3BxkZGdx38Q0M+v25LC9Yydt/fYFX3p/GZ18vKk0z8vjhfPb1Qk6+cRTZjXfmo0en8sybk9hcuBmAgVefxdrv1ifrErZJqgfUVBt38KOZdY9bllaQrgnw69or1tab/cU8dmndnk457ahbpy4n9zuWV96fVibN5PencfoRg5FEr9278+3G71ixbjU5TVuwX5c9AWjUoCHd2nUmf+0qIPaDteGHjQB898MGWjVrUbsXVgPmfrmAjjlt6NCqNXXr1OFXfY7g9Vlvl0mT3WRnunfdnTqZZesCLZtms3fnbgA0rN+ALm07snJtpXMBp6UDuu3DohVfsXTlMjYXbub5GZM5rveRZdIYRsP6OwKwY70dWb/hWwqLKn1ZZ5qI3ZQKsyRLqgXUMiQ1lDRN0oeSPpZU8prXO4DOkuZKuquSdEmXX7CKts1zStfbZLdiRRAUS9OsXUXb7C1pWme3YkVB2TRfrcrjo0ULOKDbvgD8aeTvue6xu9j1zP78/tE/cfM5v63Bq6gdK9etoXX2ll8MOc2as3Jd9YPislUrmL/4C/bbNfE16+mvdbOW5K1ZWbq+vGAlbZq1LJNm7MR/slv7zix+6m1mj53ElWNvwyx2m8bMmDRmPO/cP4HzjhlWq2WPQnRvQKkZKdXkB+pLmht8XgIMBU4ws+8kZQPvS5oIXAPsZWbdASRllZfOSn6Kkqi8AijhYbryShn/W3bjj99z+q2j+dPIa9lpx1jvxaOTn+bOEdcwpM/RvDDzVS6673om3/54lEWvdeV+D9XM4/sff2Dknddz4/mjadRgx0jKlUrKq30l/pgP6NGHeYsWMPDqs9glpz2Tb3+cdz7JZcMP33P4b09jxbrVNG/clFduf4LPly3inU9m/yzPVCSibfJLGgj8hdgrUB41szsS9l8FnBGsZgG7A83NbF1FeaZaDTW+yX8Cse9wjKR5wH+IvUu7ZTnHhU235QBphKTZkmYXFNRc07BNdkvy1qwoXV9esPJnzfM22S3JK9iSJj8uzebCzZx+62iGHXY8gw/Z8jaGf/3npdL1E/sOZM7n82rsGmpLTrPm5BesLl1fsXYNLZtmhz5+c2EhI+68niH9BnBM7341UcSkW16wkrbNW5Wut8luRf661WXSnHnUSbz8zhsALF7xNUtX5tGtbWcAVgRp13y7jonvvsEB6XQjU5ChjFBLlVlJmcCDwDHAHsBpkso0aczsrpJ4BFwLzKgsmELqBdREZwDNgR7BRa0C6m1DulJmNs7MeppZz+zs8P9pq6vHrnuzKP8rlq7MY9PmTfx7xhQGHXR4mTSDDjqcp6a9jJnxwYK57LRjI3KatsDMuOi+6+nWrjOjTzy3zDE5zVrw1scfADB97vt0btOhxq6htuzbdTeWrsjj61X5bNq8mYlvT2NArz6hjjUzrnrgDrq27ciIwafWcEmTZ/bnH9OldUc6tGxLnaw6DO03iMkJffLLVufTf7/eALRo0oxd2+7CkpXLaLBD/dK+1QY71OfI/Q9h/tIva/0atkWEfai9gIVmttjMNgHPAJV1FZ4GPF1VpqnW5E/UGFhtZpslHQaURI0NQKMQ6ZIuKzOLey76A4OvP5+iomLOOuok9ujQlUcnPwPABYNO5egD+jE1dyZ7n3cU9evV45HLxwDw3vwPeXray+zZcVcOungIADeefTkDe/XjgdG3cNUjt1FYVES9ujvwwOibk3WJkcnKzOKWCy9n+E1XUFRUzLAjB9GtfSf+8dpLAJw5cAir169l0JUXsvGH78lQBo9Nep7/3v8PFixdxAvTp7Jbh104+rLYL5+rh4/g8J69k3hF0SsqLuLyh25m0m2PkZmRyZOv/5sFXy3kgmNjv0QenfIMdzz1EOOuuIPchychievG38Xa79bTsVU7nv3jgwBkZWby7JuTeGPOW5WdLuVUo8WfLSm+L2OcmcU/X9YGWBa3ngccWP451QAYCFxSZflSoJuxVOIjTkF/6CSgDjAXOAQ4xsyWSnoK2Ad4FbizknRVPja1f4/97e1ZM2vgirYP635ak+wipLx2Q/omuwip7z/L55hZz609PCunoTU6b69Qab8ZM6vSc0kaChxtZhcE62cCvczsN+WkHQYMN7PjqyxjqNLVksTAF7wDu9wqhpmdnrCponRJewbVORetCG9K5QHt4tbbAvkVpD2VEM19SLGA6pxzFRJkRDcfai7QVVInYDmxoJlYSUNSY6AfMDxxX3k8oDrn0kKUj02ZWaGkS4CpxB6bGm9m8yWNCvaPDZKeALxuZt+HydcDqnMubUT5HKqZTQGmJGwbm7D+BPBE2Dw9oDrn0kRyh5WG4QHVOZcefLYp55yLTorHUw+ozrn0IGLTF6YyD6jOubSRkeJVVA+ozrn0kOSp+cLwgOqcSwvyu/zOORedxLmEU40HVOdc2vAaqnPORSTCsfw1wgOqcy4tyB/sd865qPhNKeeci4wHVOeci0iKx1MPqM659CD50FPnnIuMN/mdcy4iKR5PSe36s3POlYrd5Q+zhMpNGijpc0kLJV1TQZr+kuZKmi9pRlV5eg3VOZc2omryS8oEHgQGEHsDaq6kiWb2aVyaJsBDwEAz+1pSi6ry9Rqqcy4tlDzYH1ENtRew0MwWm9km4BlgcEKa04EJZvY1gJmtripTD6jOubSRkaFQC5AtaXbcMiIhqzbAsrj1vGBbvF2BnSVNlzRH0llVlc+b/M659BG+yV9gZj0ry6mcbZawngX0AI4A6gPvSXrfzL6oKFMPqM65NBHp0NM8oF3celsgv5w0BWb2PfC9pJnAvkCFAdWb/M659KCSftSqlxByga6SOkmqC5wKTExI8zLQV1KWpAbAgcCCyjL1GqpzLi2I6O7ym1mhpEuAqUAmMN7M5ksaFewfa2YLJL0GzAOKgUfN7JPK8vWA6pxLG1GOlDKzKcCUhG1jE9bvAu4Km6cHVOdc2vAJpp1zLgrVGAWVLB5QCWaxkd+fq8i6nwqSXYTUl10v2SXY7kXZh1pTPKA659KGB1TnnIuIB1TnnIuC/KaUc85FQv6SPueci44HVOeci0iKx1MPqM65NCGvoTrnXHQ8oDrn3LYTkOl3+Z1zLgp+l98556IhyPCA6pxz287H8jvnXIRSfQqjCgOqpPv5+UurSpnZ6BopkXPOlSN2Uyq1Q2plNdTZtVYK55yrkiLtQ5U0EPgLsVegPGpmdyTs70/svVJLgk0TzOzmyvKsMKCa2ZMJme8YvP3POedqX4QP9kvKBB4EBhB7u2mupIlm9mlC0rfM7Liw+VZZf5bUW9KnBG/7k7SvpIfCF90557adiAWsMEsIvYCFZrbYzDYBzwCDt7WMYc59H3A0sBbAzD4CDt3WEzvnXHVlSKEWIFvS7LhlREJWbYBlcet5wbZEvSV9JOlVSXtWVb5Qd/nNbFlCVbsozHHOORelajT5C8ysZ2VZlbMt8Sb8h0AHM9so6VjgJaBrZScNU0NdJulgwCTVlXQlQfPfOedqi4BMKdQSQh7QLm69LZAfn8DMvjOzjcHnKUAdSdmVZRomoI4CLiZWHV4OdA/WnXOuFoVr7od8EiAX6Cqpk6S6wKnAxDJnk1opqBJL6kUsXq6tLNMqm/xmVgCcEaaEzjlXUxTh0FMzK5R0CTCV2GNT481svqRRwf6xwMnARZIKgR+BU82swmfzIURAlbQLsWe1DiLWx/AecLmZLd6WC3LOueqKcuhp0IyfkrBtbNznB4AHqpNnmCb/U8BzQA7QGngeeLo6J3HOuShE2OSvmfKFSCMz+4eZFQbLP6lkSKpzztUEVWNJlsrG8jcNPr4p6RpiD74aMAyYXAtlc865OCIrjcfyzyEWQEsC/si4fQbcUlOFcs65RErnd0qZWafaLIhzzlVlu5hgWtJewB5AvZJtZvb3miqUc86VJ7XDabjHpm4A+hMLqFOAY4C3AQ+ozrlaI1K/hhqmh/dk4AhgpZmdC+wL7FCjpXLOuZ8RmRkZoZZkCdPk/9HMiiUVStoJWA3sUsPl2q68PnsmVz58G0XFRZwzcChXDRtZZr+ZccXDtzI1dwYNdqjPuCvuYL+usYltRv75Wl6d9SbNmzRjziNlH6546OW/M3biv8jKzGRgr/6MueB3tXZNNeWduR9y5xOPUVxczAmHH8n5Q04qs3/J8jz++PD9LFiymN+cegZnHz+kdN+/pkzihWlvYMBJhw9g+KDja7fwtWTAPodw95lXk5mRwRPTJ3D3pPFl9l8+6ByGHXwsAFkZWezWphPtLurH+u+/4+Kjz+Dc/ichweNvTuCBqf9MxiVslZLp+1JZmIA6W1IT4G/E7vxvBD4Ik7mk64DTic1OVQyMNLNZIY8dBfyQ7n21RUVFXPbgTUwe8zhtslvRZ/RJHHfQEezeoUtpmqm5M1iUv5RPxr/BB599xOgHbuCtv/wbgDMHnMio44dzwd1lg+WMj97nlfemkfvwJHaoW5fV31Q6xDgtFBUXMWb8OB657kZaNmvG6df+jv49e9G57ZY5LHZq2JCrz7mAN2eX/TH68uuveGHaG/xrzF3Uycri12Nupu/+PeiQ07q2L6NGZSiD+87+PYPuGMHydat4++aneWXOdD7L3zJw8d7JT3Dv5CcAOHa/fvxm4Jms//479mjbhXP7n0TfG05nU+FmJv7uYV6dO5NFq75O0tVUUxrc5a8y4JvZr83sm2BI1gDg7KDpXylJvYHjgP3NbB/gSMrOP1jZsVlmNjbdgylA7ufz6JzTgU457albpy5D+w3ilff+UybNK+9N4/QjTkASB+7enW83bmDF2tUA9Nn7AJo2avyzfMe98jRXnjKCHerWBaBFk2Y1fzE17JOFX9KuZQ5tW7aiTlYdBh7ch+m5ZX93N2vchL26dCUrs2xdYMnyPPbp2o36O+xAVmYmPfbYk/9+EOp3d1o5oPNeLFr1NUvXLGdzUSHPv/8ax/U4rML0p/Q+hufeexWA3Vp34oNF8/hx0/9RVFzEW5/NZnDPI2qr6JFI25FSkvZPXICmQFbwuSo5xOYk/Alik6yYWb6kHpJmSJojaaqknOB80yWNkTQDuFTSjcFUgUjqLul9SfMkvShp57hjegafsyUtDT7vKekDSXODYyqdw7Am5a9dRdvmrUrX22S3YvnaVZWnad6S/IQ0iRYuX8I782fT99KTGXDVGcz+fF60BU+C1evW0arZltnRWjRrxqr14WreXdq1Z85n8/lmw3f8+NNPvP2/OaxcW1BTRU2a1ju3JG/dlp+N5etW0WbnFuWmrV+3HgP2OYSXct8AYH7eQvp025+mDRtTv249Bu7bl7bNWtZKuaNQclMqlQNqZU3+eyrZZ8DhVeT9OvBHSV8A/wGeBd4F7gcGm9kaScOA24DzgmOamFk/AEk3xuX1d+A3ZjZD0s3ADcBllZx7FPAXM/tXMDVXZmKCYAbvEQDt2rdL3B2Z8ianSWy2hEmTqLCoiPUbvmPmfc8z+4t5DB9zGQuemJbyTaLKlPs9hHxQZpe27Tj3Vycy8tabaFCvHrt26EhW5s/+2dNeef+8VsFI8EH79eO9L+ay/vvvAPg8fwn3vPI4r1wzju//7wfmff05hUXpNVd8qv98V/Zgf8XtiBCCWa57AH2Bw4gF1FuBvYA3gi8mE1gRd9iziflIakws0M4INj1JbIKWyrwHXCepLbE3FX5ZTvnGAeMAevTcv8bmJmiT3Yq8NStL15cXrKR10xaVp1mzipym5dc64o8ZcshRSOKAbvuSkSEKvl1P8yZNKz0ulbVs1qxMrXL12rW02Dn89Zx4+JGcePiRAPz16X/Ssmn6d4MkWr5uFW2bbqlVtmnakvz1a8pNO7T3QJ4PmvslnpzxIk/OeBGAm04ZzfJ1lbeEUovIVGrflqrR0plZkZlNN7MbgEuAk4D5ZtY9WPY2s6PiDqnuW1UL2XIN8YMOngJ+RWwOw6mSqqpN15ie3fZmYf5Slq5cxqbNm3h+xmQGHVS232rQQYfz1LQXMTNmLZjLTjs2JKdZ5QH1+IOPZPpH7wPwZd4SNm3eTHbjnWvsOmrDnp278vXKFeStXsXmws289u7b9Ot5QOjj1377DQArCtYw7YP3OeaQvjVU0uSZvXg+XVp1oEPzNtTJzGLoQQOZ/OH0n6XbqX5D+uzWk0kfvllme/OdYr+g2jVrxeCeR/Dcu1N+dmyqKpkPNV2b/NtEUjegOK522J3Yq1OOktTbzN6TVAfY1czmV5SPmX0rab2kvmb2FnAmUFJbXQr0IPbUwclx594FWGxmfw0+7wP8N9orDCcrM4t7f/1Hjr/ufIqKizj7qJPZo2NX/jY5NgPihYNOY2Cv/kzNncGe5x1Jgx3q88hvby89/qzbL+eteR9Q8N16Og/vyx+Gj+acgUM5+6iTGPnn39Nj5CDqZtXh0SvvTPnmUFWyMjO59rwLuWjMTRQXFzOk/xF0adee5954DYBTBgyk4Jv1nHbtVXz/4w9kSPxzyiu8eM9fadigAVf8+U98u2EDWZlZ/P68EezUsGGSryh6RcVFXP7kGCb97mEyMzJ5csZLLFi+iAsOHwrAo/+NNd5+1fNwpn38Lj/89GOZ45++9M80bdiYzYWFXPbkGL75YUOtX8O2CNsFlCyqYgLqrc841ty/H2hCrCa5kFifZVvgr0BjYgH9PjP7m6TpwJVmNjs4/kZgo5ndLak7MBZoACwGzjWz9ZJ2IzZX60ZiAXO4mXWUdC0wHNgMrARON7N1FZW1R8/97Z1Zb0f7BWxHvvi2wt93LrDvxacluwip75lFc6p4cV6lcnbPsfMePz9U2jG9b9umc22tMENPRewVKLuY2c2S2gOtzKzSZ1HNbA5wcDm7CijnNdRm1j9h/ca4z3OJvTEg8ZjPiNU+S1wfbL8duD0xvXMufYlom/OSBhJ7G0km8KiZ3VFBugOA94FhZvbvyvIM04f6ENAbKPkVvAF4MGyhnXMuKiIj1FJlPlImsTh2DLF5Sk6TtEcF6e4k9u6pKoXpQz3QzPaX9D+AoKldN0zmzjkXpQjH6fcCFpa8G0/SM8Bg4NOEdL8BXgBC3R0NU7rNQZS24MTNiQ0jdc65WqNq/AGyJc2OW0YkZNeGsiM384JtW84ntQFOIHb/JpQwNdS/Ai8CLSTdRuxu+vVhT+Ccc5Go3mukC6q4KVVeRol36O8DrjazorBP0FQZUIPRRnOITeEnYIiZLQiVu3PORSjCRwPzgPghkm2B/IQ0PYFngnNmA8dKKjSzlyrKNMxd/vbAD8Ck+G1mliZT1Djntgex6fsi60PNBbpK6gQsB04lNjNeqfjXQEl6AnilsmAK4Zr8k9nysr56QCfgc2DP8GV3zrltJTIiuillZoWSLiF29z4TGG9m84NpQwlm16u2ME3+vePXg5mmRlaQ3DnnakxGhCOlzGwKsdc6xW8rN5Ca2Tlh8qz20FMz+zB40NU552qNSOPZpkpI+m3cagawP1D+9DbOOVdTqneXPynC1FAbxX0uJNan+kLNFMc55ypS+oxpyqo0oAYP9Dc0s6tqqTzOOVeu2Iz9qT0faoUBNXivU2HI150451yNS9uASmyO0f2BuZImEpslv3QCaDObUMNlc865OMmdPDqMMH2oTYG1xN4hVfI8qgEeUJ1ztUak/gTTlQXUFsEd/k/YEkhL1Ng7mJxzriLpXEPNBBoSbhIB55yrWQKlcR/qCjO7udZK4pxzlUrvx6ZSu+TOuV8UEekE0zWisoB6RCX7nHOu1kU5lr8mVBhQK3tLqHPO1bbtYiy/c86lBqX1TSnnnEspadvkd865VCKl99BT55xLIUr5PtTUDvfOORcnA4VawpA0UNLnkhZKuqac/YMlzZM0N3gVdZ+q8vQaqnMuLcTu8kdTBwymJn0QGEDsDai5kiaa2adxyaYBE83MJO0DPAfsVlm+XkN1zqUJhf4TQi9goZktNrNNwDPA4PgEZrbRzEqG2e9IiCH3HlCdc2lDUqglhDbAsrj1vGBb4vlOkPQZsTeVnFdVph5QnXNpI0MZoRYgO+j3LFlGJGQVatInM3vRzHYDhgC3VFU+70N1VcpSnWQXIfVtKkp2CbZ7olrPoRaYWc9K9ucB7eLW2wL5FSU2s5mSOkvKNrOCitJ5DdU5lx5CNvdDNvlzga6SOkmqC5wKTCx7OnVRkFnwKqi6xCbbr5DXUJ1zaUMR1QGD9+VdAkwlNvfzeDObL2lUsH8scBJwlqTNwI/AsLibVOXygOqcSxtRPthvZlOAKQnbxsZ9vhO4szp5ekB1zqUFITJ96KlzzkUjnWfsd865lJLqY/k9oDrn0kLsNdLe5HfOuQik/mxTHlCdc2nDJ5h2zrkI+ATTzjkXIW/yO+dcJOQ3pZxzLioZXkN1zrltF3tsygOqc85FwvtQnXMuEvK7/M45F4XYBNMeUJ1zbtvJm/zOOReR0G80TRoPqM65tOE1VOeci0A69KGmdumccy6eFG4JlZUGSvpc0kJJ15Sz/wxJ84LlXUn7VpWn11Cdc2kiuj5USZnAg8AAYq+UzpU00cw+jUu2BOhnZuslHQOMAw6sLF8PqM65tBFhH2ovYKGZLQ7yfQYYDJQGVDN7Ny79+0DbqjL1Jr9zLm0o5B8gW9LsuGVEQlZtgGVx63nBtoqcD7xaVfm8huqcSxvVaPIXmFnPSrP6OSs3oXQYsYDap6qTekB1zqUFRTv0NA9oF7feFsj/2TmlfYBHgWPMbG1VmXqT3zmXNqrR5K9KLtBVUidJdYFTgYllziW1ByYAZ5rZF2Ey9Rqqcy49RDj01MwKJV0CTAUygfFmNl/SqGD/WOCPQDPgoeC8hVV0I3hAdc6ljyiHnprZFGBKwraxcZ8vAC6oTp4eUJ1zaUH40FMHvD57Jlc+fBtFxUWcM3AoVw0bWWa/mXHFw7cyNXcGDXaoz7gr7mC/rnsCMPLP1/LqrDdp3qQZcx6ZXHrM8DGX8mXeEgC+2biBJg0bMeuhMl1AaemtuXO44/FxFBUXc9IRR3HhkKFl9i9evozrH7qPT5cs4tJTz+LcX50IwJL8PK64987SdHmrV3LJKcM5a9DgWi1/bRiwbx/uPvcaMjMyeWLaC9z98qNl9u9UvyHjR99Ju2Y5ZGVmct+kx/nH9JcAGHvRLRyzfz/WfLuOnlcOqf3Cb5PUnxwlqTelJF0naX4wtGuupAMlTZfUM9g/RVKTKvIoTZ+wvbukY2uo6KEVFRVx2YM38fKtf+N/46bw/PRXWPDVwjJppubOYFH+Uj4Z/wYPXHoLox+4oXTfmQNO5OVbH/tZvv/8/V+Y9dBEZj00kSF9jmLwIUfV+LXUtKLiIm577GHG/v4mJt77EFPemcHCvK/LpGncsBHXnjuSc48/scz2Tq3bMuGu+5lw1/08f+d91Ku7A0f26l2bxa8VGcrgvvOvY/CYUex3+a8Yesix7Namc5k0Iweexmd5izjwdydy9I3ncMdZv6NOZh0A/jH9JQaPGVle1mkhQxmhlqSVL1knltQbOA7Y38z2AY6k7IO2mNmxZvbNVp6iO5D0gJr7+Tw653SgU0576tapy9B+g3jlvf+USfPKe9M4/YgTkMSBu3fn240bWLF2NQB99j6Apo0aV5i/mfHCzFc5pf9xNXodteHjhV/QrlUO7Vq2om5WHY49+FDezH2/TJpmjZuwd5ddycrMrDCf9z/+iHatcmjdvEVNF7nWHdBlbxatXMbS1XlsLtrM8+9O4bgDDiuTxsxoWG9HAHas14D1G7+lsLgQgHcWzGHdxm9rvdxRifAuf41IZg01h9jDtz8BmFmBmZV5DkzSUknZwec/SPpM0huSnpZ0ZVzSoZI+kPSFpL7BYxA3A8OCmu+w2rqoRPlrV9G2eavS9TbZrVi+dlXlaZq3JD8hTUXe+WQ2LXfOpkubjpGUN5lWrVtLTrPmpestm2Wzal2Vj/79zKvvzOTYQw6Nsmgpo3XTluStXVG6vnztKto0bVkmzdjXnmK3Nruw+JHpzL7nJa58/HbMyn1mPa2UvKTPA2r5XgfaBUHwIUn9KkoYNOlPAvYDTgQSm/hZZtYLuAy4wcw2EXvk4Vkz625mz9bIFYRQ3g9yYsd6mDQVeW76KwztP2jrCpdqyvk/X92bEJsKN/PmnA84+qAqB7WkpfK+jsSfnwH79mHeV5+xy8j+HHjVSdx7/nU0qr9jLZWwJgkp3JIsSQuoZrYR6AGMANYAz0o6p4LkfYCXzexHM9sATErYPyH4ew7QMcz5JY0oGee7Zk1BdYsfWpvsVuStWVm6vrxgJa2btqg8zZpV5DSturlaWFTIy++8zsmHbh8BtWWzZqxYu6Z0fdXaAlrs3LRaebz9vzns0akz2U12jrp4KWH52lW0bZZTut6mWUvy168uk+bMw4bw8qw3AFi86muWrl5Ot9a71Go5a45CLsmR1JtSZlZkZtPN7AbgEmK10PJU9Q39FPxdRMgnF8xsnJn1NLOezZtnhyvwVujZbW8W5i9l6cplbNq8iednTGbQQUeUSTPooMN5atqLmBmzFsxlpx0bktOs6oD63/+9y67tdinTXZDO9uq8K1+vyCdv9Uo2FW5myrszOaxnpbOl/cyUd2Zst819gNmLPqFLTns6NG9Dncw6DD34WCbPfrNMmmUFK+i/90EAtGjcjF1bd2TJ6mXlZZdelPo3pZL22JSkbkCxmX0ZbOoOfAXsVU7yt4FHJN1OrMyDgL9VcYoNQKNoSrv1sjKzuPfXf+T4686nqLiIs486mT06duVvk58G4MJBpzGwV3+m5s5gz/OOpMEO9Xnkt7eXHn/W7Zfz1rwPKPhuPZ2H9+UPw0dzzsDYo0TPT5+8XdyMKpGVmcl1541ixG1/pLi4mBMOG0CXdh149vXYs9fDjjqWNd+sZ9g1l7Hxxx/IUAb/mPIyE//8MA0bNODHn/6Pd+fN5YYRlyT5SmpOUXERl4+/jUnXjSMzI4Mn33yRBXmLuGDAKQA8+sZz3PHCWMb9+jZy734RIa77159Zu+EbAJ689C767nEA2Y2asPDhadzy3IM8+eaESs6YWlL9sSklq7NaUg/gfqAJUAgsJNb8/zdwpZnNlrQU6GlmBZJuBE4jFnTXANPN7G+SpselzwZmm1lHSU2JDSurA9xeWT9qj5772zuz3q6hK01/i78LNYz5F23PC05IdhFS34Slc6oaulmZffbf2ybODBf8OzXadZvOtbWSVkM1sznAweXs6h+XpmPc9rvN7EZJDYCZwD1Bmvj0BQR9qGa2Djgg4mI755Io1Wuo6TRSapykPYB6wJNm9mGyC+Scq10eUCNiZqcnuwzOueTysfzOOReBiCeYrhEeUJ1zacOb/M45FxkPqM45F4nUDqf+TinnXBqJciy/pIGSPpe0UNI15ezfTdJ7kn5KmIypQl5Ddc6lkWjqqJIygQeBAcTegJoraaKZfRqXbB0wGhgSNl+voTrn0kTYyftCBd1ewEIzWxzMTvcMUOb1Dma22sxygc1hS+g1VOdcWlD13nqaLWl23Po4MxsXt96GshPa5wHVm4mnHB5QnXPbo4IqxvKXF5m3eWITD6jOubQR4XOoeUC7uPW2QH4FaUPzPlTnXNqIsA81F+gqqVPwyqRTgW1+bbDXUJ1zaSOqsfxmVijpEmJTfGYC481svqRRwf6xkloBs4GdgGJJlwF7mNl3FeXrAdU594tkZlOAKQnbxsZ9XkmsKyA0D6jOuTSR3DeahuEB1TmXRjygOufcNkvu+0zD8YDqnEsbPsG0c85FxPtQnXMuMh5QnXMuAuGn5ksWHynlnHMR8Rqqcy4txO7yp3YN1QOqcy6NeEB1zrlIZKR4H6oHVOdcmkj9R/s9oDrn0kZqh1MPqM65tJLaIdUDqnMuPVTvnVJJ4QHVOZcW0uGxKZlt83up0p6kNcBXyS5HnGygINmFSHH+HVUuFb+fDmbWfGsPlvQasesKo8DMBm7tubaWB9QUJGl2FW9s/MXz76hy/v0khw89dc65iHhAdc65iHhATU3jkl2ANODfUeX8+0kC70N1zrmIeA3VOeci4gHVOeci4gG1FkkqkjQ3bulYQbomkn4dMs+NkRayFki6TtJ8SfOC7+HAahw7StJZNVm+VFTedyZpuqSewf4pkppUkUdp+oTt3SUdW0NF/0XxkVK160cz6x4iXRPg18BDNVqaJJDUGzgO2N/MfpKUDdQNeWyWmY2t0QKmoDDfmZltS0DsDvQEpmxDHg6voSaVpIaSpkn6UNLHkgYHu+4AOgc1kbsqSZeOcoiNYvkJwMwKzCxfUg9JMyTNkTRVUg6U1qrGSJoBXCrpRklXBvu6S3o/qLW9KGnnuGNKam7ZkpYGn/eU9EHwvc6T1DUJ1781yv3O4hNIWhoEWiT9QdJnkt6Q9HTJ9xUYGnwHX0jqK6kucDMwLPhehtXWRW2XzMyXWlqAImBusLxIrIWwU7AvG1hIbMhyR+CTuOPKTResb0z2dVXzO2gYXP8XxGrg/YA6wLtA8yDNMGB88Hk68FDc8TcCVwaf5wH9gs83A/fFHdMz7vtaGny+Hzgj+FwXqJ/s72Nrv7NyrnNpcK09g7T1gUbAl3Hf13TgnuDzscB/gs/nAA8k+zq3h8Wb/LWrTJNfUh1gjKRDgWKgDdCynONUQbqVNV7iiJnZRkk9gL7AYcCzwK3AXsAbwWxCmcCKuMOeTcxHUmOgiZnNCDY9CTxfxenfA66T1BaYYGZfbsu11JbyvjNJ11SQvA/wspn9CCBpUsL+CcHfc4j94nYR8oCaXGcAzYEeZrY5aJrW24Z0acHMiojVlqZL+hi4GJhvZr0rOOT7ap6ikC3dWaXfk5k9JWkWMAiYKukCM/tvNfNOinK+s7MrSFrVdEw/BX8X4f//I+d9qMnVGFgdBMnDgA7B9g3EmmtVpUs7krol9F12BxYAzYObL0iqI2nPyvIxs2+B9ZL6BpvOBEpqq0uBHsHnk+POvQuw2Mz+CkwE9tm2q6kdFXxnFc2O9jZwvKR6khoS++VRlcSfN7eVPKAm17+AnpJmE6uFfgZgZmuBdyR9IumuitKlqYbAk5I+lTQP2AP4I7HAd6ekj4j1AR4cIq+zgbuCfLoT60cFuBu4SNK7lJ3ubRjwiaS5wG7A37f5ampHed/ZjeUlNLNcYr8sPiLWvJ8NfFtF/m8Ce/hNqW3nQ0+d285Iahj0uzYAZgIjzOzDZJfrl8D7UJzb/oyTtAex/uMnPZjWHq+hOudcRLwP1TnnIuIB1TnnIuIB1TnnIuIB1YWiLTNlfSLp+eAO8tbm9YSkk4PPjwY3UCpK219SmEeoEo8rHdseZntCmmrN4BU/v4D7ZfOA6sL60cy6m9lewCZgVPxOSZlbk6mZXWBmn1aSpD/hnkl1Luk8oLqt8RbQJag9vinpKeBjSZnB7Fi5wWxOIwEU80DwYPpkoEVJRgkzQw0MZtT6SLHZtToSC9yXB7XjvpKaS3ohOEeupEOCY5tJel3S/yQ9QtVDMJH0kmKzW82XNCJh3z1BWaZJah5s6yzpteCYtyTtFsm36bYb/hyqqxZJWcAxwGvBpl7AXma2JAhK35rZAZJ2IDba63VgP6AbsDexSV0+BcYn5Nsc+BtwaJBXUzNbJ2kssRm17g7SPQXca2ZvS2oPTAV2B24A3jazmyUNAsoEyAqcF5yjPpAr6YVglNqOwIdmdoWkPwZ5X0LsxXejzOxLxSbFfgg4fCu+Rred8oDqwqofDNmEWA31MWJN8Q/MbEmw/Shgn5L+UWJzEHQFDgWeDib4yJdU3oQkBwEzS/Iys3UVlONIYsMkS9Z3ktQoOMeJwbGTJa0PcU2jJZ0QfG4XlHUtsRm9Sma4+icwIRgXfzDwfNy5dwhxDvcL4gHVhfWztw0EgSV+JigBvzGzqQnpjgWqGkGiEGkg1k3Vu2R6uoSyhB6lIqk/seDc28x+kDSdimfwsuC83yR+B87F8z5UF6WpxCYlqQMgaVdJOxIbT35q0MeaQ2xOz0TvAf0kdQqObRpsT5wJ6XVizW+CdN2DjzOJTRyDpGOAnasoa2NgfRBMdyNWQy6RwZZZqk4n1pXwHbBE0tDgHJK0bxXncL8wHlBdlB4l1j/6oaRPgEeItYJeJDZz/MfAw2yZZq+Uma0h1u85IZhxqqTJPQk4oeSmFDCa2Mxb8yR9ypanDW4CDpX0IbGuh6+rKOtrQFYwe9MtwPtx+74H9pQ0h1gfacksVmcA5wflmw+k86toXA3wsfzOORcRr6E651xEPKA651xEPKA651xEPKA651xEPKA651xEPKA651xEPKA651xE/h+B9spPOFno+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing confusion matrix from sklearn.metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(knn_u, Xtest, ytest,\n",
    "cmap=plt.cm.Greens, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in above confusion matrix we can see that only slight accident predicted with good accuracy of 81% and serious and fatal with 19% and 12% respectively. 17% of slight accident predicted as serious, and 79% of serious accident predicted as slight and 85% of fatal accident as slight which is not good for prediction.\n",
    "\n",
    "Most of the accident predicted as slight by this model, which is a major concern to look out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Descision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descision Tree:\n",
      "Precision: 0.3573773357240322\n",
      "Recall: 0.34047023715351576\n",
      "F score: 0.32744408598551417\n"
     ]
    }
   ],
   "source": [
    "#Importing precision, recall and fscore from sklearn.metrics library\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "yhat = des_tree_u.predict(Xtest)\n",
    "\n",
    "# micro-averaged precision, recall and f-score\n",
    "p, r, f, s = precision_recall_fscore_support(ytest, yhat, average=\"macro\")\n",
    "print(\"Descision Tree:\")\n",
    "print(f\"Precision: {p}\")\n",
    "print(f\"Recall: {r}\")\n",
    "print(f\"F score: {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see F score is very low only 32.74% of test records are predicted corretly, the score of predicting positive value (precision) is also very low only 34.00% of positive records predicted correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fe32779c550>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAAEGCAYAAAA61G1JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsrElEQVR4nO3deZxN9f/A8dd7FiFJzBiMNZGIxCDKkpA134qSViVNJapv27fF1vYtLSKypa9+329ZSt+ILCmkohllzdLQ0Bjb0GIrzLx/f9wz495ZL86de2/f99PjPNxzz/t8zucc4z2f8znnfI6oKsYYY85cRLArYIwxfxWWUI0xxiWWUI0xxiWWUI0xxiWWUI0xxiVRwa5AKIiJidHqNaoHuxohK4usYFch5K3ZujHYVQh9vx3LUNXY011dYkoqx/z8WTx4fIGqdj7dbZ0uS6hA9RrVWb5yWbCrEbKOZh4JdhVCXswNzYNdhdA3e/v2M1r/WBa0qOhf7Gc7Y85oW6fJEqoxJnyIBLsGhbKEaowJDwJEWkI1xhh3hHY+tYRqjAkXYqf8xhjjCiHkb/S0hGqMCR/WQjXGGJeEdj61hGqMCRN2ld8YY1xkp/zGGOOS0M6nllCNMWFCgIjQzqiWUI0x4SO086klVGNMmBCByNC+EdUSqjEmfFgL1RhjXGJX+Y0xxiWhnU8toRpjwoRd5TfGGBeFdj61hGqMCSP26KkxxrhAbDxUY4xxT2jnU0uoxpgwYi1UY4xxSWg/KGUJ1RgTJuy2KWOMcZElVGOMcYn1oRpjjAsEu8pvjDHuEMTPFqoGuCYFsYRqjAkboZ5QQ/wmBGOM8RAgMkL8mvwqT6SziGwWkRQReSKf5eeKyBwRWSMiG0SkX1FlWkItBguTv6Rx/840vLMTr8yYmGe5qvLIW8/R8M5ONL/3Gr5P2ZCzLPG1J6nRpxUJiT3yLXvUB29zdpd6ZPz2S8DqH2iLV31Fi3uupdnd1/DGzHfyLFdV/jHhZZrdfQ1tBt7AmpSNOct+O3SQfi88ymWJ19Ey8TqSNq4BYP22LXT+++20vv8G+g4fzMEjh4ptfwKt46VXsGbMXNaPnc8j1/bPs7zc2WWZ/vhovn3tI758aRr1q18AQNUKlZg//B2+Hz2HVaNmc3+3W4q76mdGPC1Uf6YiixKJBMYCXYD6wE0iUj9X2P3AD6p6CdAOeFVEShRWbkglVBHJFJHVXlPNAuLKich9fpYZ1P9JmZmZPDx2BB89O4lVEz5h5pK5bNye4hOzIGkZKenbWfv2At4cNIIH3xyes+yWjtfy3+cm5Vt22r5dfP7911SrWCWg+xBImZmZPP7WS0wfPoavxn3IrKXz2bxjm0/MZ8lfsS19B99O/JjXBj7No+NezFn25MSRtG/aihXjZ7F0zHTqVjsfgAfHjOCZOwbx5dgZdGt5JW9++G6x7legREREMOrup+n53D1cOrgHvVt3pV7V2j4xj10/gDU/baL5w9dy1+h/8MqdTwJwIusET0x9mUsH9aDtE324p0vfPOuGOrcSKtAcSFHVbap6DJgG9MwVo8A54imwDHAAOFFYoSGVUIGjqtrYa0otIK4c4FdCDbbkLWs5v0p1alWuRonoEvRq25VPViz2iZm7YjF9r+qJiND8osb8duh3dh3YC8AVDZtR/pxz8y378Qkv8txdj4b6hc9CfbdlPbUqV6VmpaqUiI7m2jZX8+mKJT4xn65cwg3tuyMiJNRrxG+HD7L7wD4OHjnENxu+45ZOfwOgRHQ055Y5B4CUtO20urgJAO0uvYw5X/se83DV7IKGbN21g9Q9aRw/cZyZyz+le/P2PjH1qtVmydoVAGzZ+RM1Klah4rkV2P1LBqu3eVr3h/44wqa0bVSpULHY9+H0+ZdMnYQaIyLJXtOAXIXFAz97zac533l7E7gISAfWAYNVNauwGoZaQvUhImVEZLGIfCci60Qk+zfIP4HaTit2ZCFxQZeesYeqsZVz5uNjKrFr/x7fmP17qBpzMqZKTCV2ZfjG5DZ3xedUjomj0fn13K1wMdu1fx9VYivlzFeJqciu/XtzxewlPibuZEyFiuzav4/U3TupUPY8Hhg1jCsH3cTg0SM4/MdRAC6qUZtPVy4F4OPln7GziOMZLqpUiCNt/+6c+Z37dxNf3jcprkvdTM/LOgCQcEFDqsdWIb5CnE9M9dgqNK51EUlb1ga+0i7KHnCqqAnIUNUEryl3X1t+7ZDc17KuBlYDVYDGwJsiUraw+oVaQi3ldbr/EfAHcK2qNgGuxNOHIcATwFanFftoIXFBl9/VRsn1b6n5BBVW/SN/HOXlaeN55tZBZ1i74NN8jlDufc//+MCJzEzWbt1Ev669+GL0+5x9VilGO32wowcPZcrcGbQf3JdDRw9TIio6IPUvbrl/diDvz9grsyZRrsy5rHh1Fvd2vZk1P23kRFZmzvKzS5bm/cfe4NEpL3Lw6OEA19g9gqun/GlANa/5qnhaot76AbPUIwX4CSi0BRNqt00dVdXG2TMiEg28ICJtgCw8TfK4fNaTAuJ25xObXfYAYABAterVCgo7Y/ExcaTt25UzvzNjN5VynWbFx8SRlnEyJj2fGG/bdu0gdXcal93X0ylzD5c/cB1LR82gUvlYl/cgsKpUqEj6vpP/TOkZe/PsQ5WYij4tzPT9nhgRoUpMRZpe2BCAHpdfxRsf/AuAOtVq8cGz4wBI2bmdRUnLA7wnxWPn/t1UrXCyRR9foRLpB3xb9AePHuaeN5/Kmd80fhGpe9IAiIqM4v1HRzF92Sd8vPKz4qm0WwQixLU2YBJQR0RqATuBPkDfXDE7gKuAL0UkDrgQ2EYhQq2FmtvNQCzQ1Em0e4CSZxCXQ1UnZp8OxMTEuFppb03rNmRr+nZSd6dx7PgxPlg6j26X+fZ5dbusPe8t/hhV5duNqyl79jlULl9wQr241oVsn/Y1G6d+zsapnxMfE8dXY2aFXTIFuLRuA7al/8z23Ts5dvw4Hy1bQOcWbX1iOrdoy4zPP0FVSd60lrKly1CpfCxx58UQHxPHj2mpACxb8y0XVq8FwL5fDwCQlZXFa9Mmc0eX64t1vwIlOWU9F1SuQY2K8URHRdP7ii7MTfrCJ+bc0ucQ7bTI+3XoxfIfknNaouPvf5bNO7cxes7UYq+7G9xqoarqCWAgsADYCMxQ1Q0ikigiiU7Ys0ArEVkHLAYeV9WMwsoNtRZqbucCe1X1uIhcCdRwvj8InONHXNBFRUbx6r3P0PPpu8jMzOK2TtdTv0YdJs+dBkD/bn24ullbFiQto+GdnShVsiQTHnohZ/3b//kwX65NYv/vv1DnlrY8fesD3H51r2DtjuuiIqP4Z+Lj9B5yP1lZWfTteA31atTmnXkfANCvay86JlzBZ8nLaXZ3T0qdVZLRDw7LWf/FxMdJfOUpjp84To1KVRnjLJu1dD5vz50BQPdW7enbMWS61c9IZlYmD01+njlDJhEZEcHUxR+x8ecU+ne6EYDJC6dTr+r5TB70TzKzMtmUtpXEsc8A0KpeE25u15N1qZtZ8eosAIb+ZxQLvlsWtP05VW525KnqPGBeru/Ge31OBzqdSpmi+XVQBYmIHFLVMl7zMcAcIBpP5/DlQBdVTRWR94BGwKfAS4XE+ZSZnyZNm+jyleHzQ1XcjmYeCXYVQl7MDc2DXYXQN3v7KlVNON3VoyqX0XPuvNiv2F9fWHlG2zpdIdVCzZ34nOZ1ywJic/d3FBRXaDI1xoSPELnWXKCQSqjGGFMggQgbD9UYY85c9m1TocwSqjEmbFhCNcYYV/g/HmqwWEI1xoQHsRaqMca4JsTzqSVUY0x4EDzDF4YyS6jGmLAREeJNVEuoxpjwIHbKb4wxrhC7ym+MMe7JbzzYUGIJ1RgTNqyFaowxLrFn+Y0xxgViN/YbY4xb7KKUMca4xhKqMca4JMTzqSVUY0x4ELFHT40xxjV2ym+MMS4J8XxqCdUYEy7sKr8xxrjGEqoxxrjAbuw3xhgX2aOnxhjjFmuhGmOMG+yilDHGuMNG7DfGGHcIdlHKGGNcYwnVGGNcYlf5jTHGDWIXpcKCCERIaI9iE0wR2LEp0h+Zwa7BX571oRpjjIssoRpjjEssoRpjjBsk9C9KWeeYMSYsiPOklD+TX+WJdBaRzSKSIiJPFBDTTkRWi8gGEVlaVJnWQjXGhA23TvlFJBIYC3QE0oAkEZmtqj94xZQDxgGdVXWHiFQsqlxroRpjwoaIf5MfmgMpqrpNVY8B04CeuWL6ArNUdQeAqu4tqlBLqMaY8OCMh+rnKX+MiCR7TQNylRYP/Ow1n+Z8560ucJ6ILBGRVSJyW1FVtFN+Y0z48P+UP0NVEworKZ/vNNd8FNAUuAooBXwjIitUdUtBhVpCNcaEBQEi3bvKnwZU85qvCqTnE5OhqoeBwyKyDLgEKDCh2im/MSZMuHqVPwmoIyK1RKQE0AeYnSvmY6C1iESJSGmgBbCxsEKthWqMCQ8CES5d5VfVEyIyEFgARAJTVHWDiCQ6y8er6kYRmQ+sBbKAyaq6vrByLaEaY8KC28/yq+o8YF6u78bnmh8JjPS3TEuoxpiwEep9lAUmVBEZQ96rXjlUdVBAamSMMfnwXJQK7ZRaWAs1udhqYYwxRRLX+lADpcCEqqpTvedF5Gzn9gFjjCl+EvqjTRXZfhaRliLyA87tAiJyiYiMC3jNjDHGi+BJWP5MweLPtkcBVwP7AVR1DdAmgHUyxph8RYj4NQWLX1f5VfXnXE1te9+DMabYhfopvz8J9WcRaQWo80TBIIp4WsAYY9wmQORfIKEmAm/gGYllJ54nC+4PZKWMMSavML7Kn01VM4Cbi6EuxhhTIHHx0dNA8ecq//kiMkdE9onIXhH5WETOL47KGWOMNzdfgRII/lzlfw+YAVQGqgAzgfcDWSljjMlPqF/l9yehiqr+n6qecKZ/U8gjqcYYEwhyClOwFPYsf3nn4xfOGwGn4UmkNwJzi6FuxhjjRYgK42f5V+FJoNkJ/x6vZQo8G6hKGWNMbhIGj54W9ix/reKsiDHGFCXUr/L79aSUiFwM1AdKZn+nqu8GqlLGGJOf0E6nfiRUERkKtMOTUOcBXYDlgCVUY0yxEUK/hepPD28vPK9R3a2q/fC89e+sgNbKGGPyECIjIvyagsWfLR9V1SzghIiUBfYCdmN/ERYmL6PRXVfToF8HRk6fkGe5qvLwuGdp0K8DzRJ78P2PG4pc9x+TXuKS/lfTLLEHN4y4j18P/Q7A+5/PpsV91+RMpbtcyJqtPwR+J13y2aqvaH5PT5re3YNRM6fkWa6qPDHhJZre3YMrBvZmTcrJoSQuubMLl9/fizYP3ED7B/vmfL9+22Y6/f02Lr+/FzcNH8TvRw4Vy74Uh45NW7Nm4nzWT17II73vzrO8XJmyTH/6Tb4dO5svX59J/Rp1fJZHRETwzZiP+HDY+DzrhrK/yvB9ySJSDpiE58r/d8C3/hQuIk+JyAYRWSsiq0Wkhb8VE5FEEbnN3/hQkpmZyYNjh/Pxc5P4fuI8Zi75hI3bU3xiFiQtZWt6KuunLOLNwc8y6M2hRa57VZPLWTVhLknj51AnvlZOsr2p/TWsHDebleNm8/ajI6kRF88ltesX706fpszMTB5760VmDB/LN+Nm8eHS+WzasdUn5rPk5WxN30HyxNm8PvAZ/j7ueZ/ls1+YxLIxM/h81Hs53w0eM5yhdwziq7Ef0K1le8Z86DNeetiKiIhg1H1D6DmkP5cmdqN32+7Uq1bbJ+axGxJZs20jze+/hrtefZxX7nnKZ/nAnrex+WffYxwW5C/wpJSq3qeqvzpvA+wI3O6c+hdKRFoC3YEmqtoI6AD87E+lRCTKeY1rWPbTJm1eS+3KNahVuTolokvQu203PvnmM5+YT75ZTN+rrkVEaHFRY347dJBd+/cWum6HplcQFenp9m5e7xJ2ZuzOs+0ZSz7hhnbdA7+TLlm1ZT21KlejZqWqlIiO5ro2V/PpiiU+MfNWLqFP++6ICM3qNeL3wwfZfWBfoeX+mLadVhc3BaDdpZcx5+vFgdqFYtWsbiO2pm8ndXcax08cZ+ayuXRveZVPTL3qtVmyegUAW9K2USMunorlKgAQXyGOzs3a8c6CD4q97m4I2yelRKRJ7gkoD0Q5n4tSGchQ1T/BM8iKqqaLSFMRWSoiq0RkgYhUdra3REReEJGlwGARGSYijzjLGovICqel+5GInOe1ToLzOUZEUp3PDUTkW6dVvFZE6uRTv4BJ37+HqrGVcubjYyqxc/+ewmNi40jfv8evdQHeXfghVyfkHef7g2Xzwiqh7tq/l3iv/a0SE8eu/XvzxsR4xVQ4GSMiXD/kXq4cfBP/mn8ySVxUozafrlwCwMfLF5Gezy+fcFSlQhxpXvuyM2MP8RXifGLW/bSJnpd3BCChbkOqV6ySc/xG3vMkT00ZSVZWVvFV2iXZF6VCOaEWdpX/1UKWKdC+iLIXAkNEZAvwGTAd+BoYA/RU1X0iciPwPHCns045VW0LICLDvMp6F3hAVZeKyAhgKPBgIdtOBN5Q1f84Y7hG5g4QkQHAAIBq1asVsSunRjXvk7m5T0MKivFn3Zfef4vIyEj6tL/G5/tvN62h9FmlaFCz7ulUOyg0n6eY/T1WAJ++/C8qV6jIvl8PcN3TidStWotWFzdlzODhPDHxJUa+P5HOLdoSHRUdmB0oZvmdzuY+Pq/MmMgriU+xYsx/2bB9C2u2buRE5gm6NG/H3l8P8H3KBlo3bF5cVXZVON/Yf+WZFKyqh0SkKdAauBJPQn0OuBhY5ByYSGCX12rTc5cjIufiSbRLna+m4hmgpTDfAE+JSFVglqr+mE/9JgITAZomNHF1bIL4mEqk7fNuReymSvmKhcfs20Pl8hU5dvx4oev+e9Es5q38gk//OTXPD9fMpXO5oV03N3cl4KpUiGOn1/6mZ+yhUvlY35iYOJ/ujfT9J2MqV/Acm9hy5enW8kpWbVlPq4ubUrdaLWY967nokrJzO4uSvgz0rhSLnRm7qRrjfQYTR/oB3xb9waOHuef1J3PmN72zmNTdafRu243ul7Wnc7M2nBV9FmVLl2HKIyO585VHi63+Z0aIlNB+9DSgtVPVTFVdoqpDgYHA9cAGVW3sTA1VtZPXKqf6VtUTnNwH74cO3gOuAY4CC0SkqNa0qxIubEhKeiqpu3/m2PFjzFw6l26X+fZzdbusPe8t/ghVZeXG1ZQ9uwyVK1QsdN2Fyct4deYkPhg2ntIlS/mUl5WVxawvP6V32/BKqE3qNmBb+g62797JsePHmbVsAZ1btPWJ6dKiLdM+/wRVJWnTWsqWLkOl8rEc/uMoB494fmQO/3GUL77/hotqXADAvl8PAJ7j8uq0SdzRpXfx7liAJG9ZxwVValIjrirRUdH0btONuSs+94k59+xzclrk/a7uzfL1yRw8epgh/3qNC25rS71+V3HbSw+zZO2KMEqmJ8dDDddT/jMiIhcCWV6tw8Z4Xp3SSURaquo3IhIN1FXVDQWVo6q/icgvItJaVb8EbgWyW6upQFM8dx308tr2+cA2VR3tfG4E+P7UBVBUZBSv3zeEHk/dRWZWJrd36kX9mnWYNNcz6uHd3W6ic/N2LEhaSoM7O1D6rFJMePjFQtcFeGjsCP48fozuT94BQPN6jRkzaAQAy9clER9TiVqVqxfXbroiKjKKlxOfoNeQe8nMyuLmjj25qMYFvDPPcxLSr2tvOia0ZlHycpre3YNSZ5XkzQeHA7Dv1/3c+tzDAJzIOkGvtl3o0PRyAD5c+ilvz/Wc8HRvdRU3d+wZhL1zX2ZWJg+9NYI5z00mMiKSqQs/ZOOOFPp37QPA5HnTqFetNpP//hKZWVls2pFC4htPFVFq+JAQf1ZK8uufcqVgz+n+GKAcnpZkCp4+y6rAaOBcPAl9lKpOEpElwCOqmuysPww4pKqviEhjYDxQGtgG9FPVX0SkHp6xWg/hSZi3qGpNEfkHcAtwHNgN9FXVAwXVtWlCE/1q5XJ3D8BfyNETR4JdhZBX/ppLg12F0LcwbZWqJpzu6pUvqqx3vnOXX7EvtHz+jLZ1uvx59FTwvALlfFUdISLVgUqqWui9qKq6CmiVz6IM8nkNtaq2yzU/zOvzauCyfNbZhKf1me1p5/sXgRcLq58xJrxIGLxTyp8+1HFAS+AmZ/4gMDZgNTLGmAIIEX5NweJPH2oLVW0iIt8DOKfaJQJcL2OMySOYz+n7w5+EelxEInFeeyIisUD43RVsjAlr4vwJZf4k1NHAR0BFEXkez9X0pwNaK2OMyS0MXiNdZEJ1njZahWcIPwH+pqobi1jNGGNcF7ZPSmVzruofAeZ4f6eqOwJZMWOM8eYZvi/8+1DncvJlfSWBWsBmoEEA62WMMbkIEeF+UUpVG3rPOyNN3VNAuDHGBExEiF+UOuV0r6rfAc0CUBdjjCmQ4O4A0yLSWUQ2i0iKiDxRSFwzEckUkV4FxWTzpw/1Ya/ZCKAJUPjovsYY4zYXr/I7t4KOxTNofhqQJCKzVfWHfOJeAhb4U64/LdRzvKaz8PSp/jVGmjDGhBHx+48fmgMpqrpNVY8B08g/rz0AfIjnXXpFKrSF6mTnMqoaPmN8GWP+kjwj9vvdSxkjIsle8xOdMZCzxeP7SqY0wOeddyISD1yLZzB9v7o5C0yoznudTvj5uhNjjAm4U0ioGUWMNpVfMzb30HujgMdVNdPfftnCWqjf4ukvXS0is/GMkp8zALSqzvJrC8YY4wpXR5tKA7zffVQVSM8VkwBMc5JpDNBVRE6o6n8LKtSf+1DLA/vxNHuz70dVwBKqMabYCK4OMJ0E1BGRWsBOoA/Q1ztAVWvlbFvkX8AnhSVTKDyhVnSu8K/nZCLN2dap1NwYY9zgVgvV6c4ciOfqfSQwRVU3iEiis3z86ZRbWEKNBMrgX1+DMcYEloC4+JI+VZ0HzMv1Xb6JVFXv8KfMwhLqLlUd4XftjDEmoMJ7+L7Qrrkx5n+KEN4DTF9VyDJjjCl2of4sf4EJtbC3hBpjTHHLfpY/lPlz25QxxoQAcfWiVCBYQjXGhI2wPeU3xphQInJKj54GhSVUY0yY8H+s02CxhGqMCRt2ym+MMS7wXOW3U35jjHFBeD8pZYwxIcX6UI0xxiV2ld+EvVD/IQ4JWTYAW6AJdlHKGGPccQqviA4WS6jGmLAhfr2oOXgsoRpjwoa1UI0xxgWCEBni/fmWUI0xYcPuQzXGGJfYKb8xxrjA8xppO+U3xhgX2G1TxhjjGrux3xhjXGADTBtjjIvslN8YY1whdlHKGGPcEmEtVGOMOXOe26YsoRpjjCusD9UYY1whdpXfGGPc4Blg2hKqMcacObFTfmOMcYm99dQYY1xjLVRjjHGB9aEaY4ybrIVqjDFuCP0+1NBuPxtjjBdxXiVd1ORnWZ1FZLOIpIjIE/ksv1lE1jrT1yJySVFlWgvVGBM23GqhikgkMBboCKQBSSIyW1V/8Ar7CWirqr+ISBdgItCisHItoRpjwoaLp/zNgRRV3QYgItOAnkBOQlXVr73iVwBViyrUEqoxJizIqT16GiMiyV7zE1V1otd8PPCz13wahbc+7wI+LWqjllCNMWHjFFqoGaqaUGhReWm+gSJX4kmoVxS1UUuoxpjw4O6jp2lANa/5qkB6nk2KNAImA11UdX9RhdpVfmNM2BA///ghCagjIrVEpATQB5jtsy2R6sAs4FZV3eJPodZCNcaEBcG9FqqqnhCRgcACIBKYoqobRCTRWT4eGAJUAMY52z1RRDeCtVADZWHyMhrddTUN+nVg5PQJeZarKg+Pe5YG/TrQLLEH3/+4och1h08dRbPEHrS47xq6P9mP9P17fMrcsTedmL815vUP3g7cjgXAZ8nLSRjQg0v7d+P1GXnrrqo8Nv6fXNq/G63uv57VKT/4LM/MzKT1Azdw47CBOd+t3bqJDg/fzBUDe9NucB9WbV4X8P0oLh2btmbN5Pmsn7KIR24YkGd5uTJlmf7MWL59azZfvvEB9WvUyVm2aernJL01hxVjP2b56A+Ls9ou8Ld96l/SVdV5qlpXVWur6vPOd+OdZIqq9lfV81S1sTMVmkwhyAlVRJ4SkQ3OjbOrRaSFiCwRkQRn+TwRKVdEGTnxub5vLCJdA1T1QmVmZvLg2OF8/Nwkvp84j5lLPmHj9hSfmAVJS9mansr6KYt4c/CzDHpzaJHrPtSrP0nj57By3Gy6NL+SF/8z1qfMxya8QKeENsWzky7JzMzkkbde4IPhb7Hyrf/ywbJP2bRjq0/MouTlbEvfzneTPuGNB4bw97HP+Sx/a/Z/uLBaLZ/vhr7zOo/3TWT5mzN58pb7GfLO6wHfl+IQERHBqPuH0vPpu7l0QFd6t+tOveq1fWIe65PImm0baX7vNdw18jFeSXzaZ3nnx2/jsvt7csWg64uz6q6IkAi/pqDVL1gbFpGWQHegiao2AjrgexsDqtpVVX89zU00BoKSUJM2r6V25RrUqlydEtEl6N22G59885lPzCffLKbvVdciIrS4qDG/HTrIrv17C1237NllctY/8scRn9Of2V8volalatSvcUHx7KRLVm1Zz/lVqlOzclVKREdzfZvOzFvxhU/MvBVf0Kd9D0SEZvUu4bfDB9l9YB8AOzN2szBpGbdefZ3POiLCwSOHAfj98EEql48tnh0KsGYXNmLrru2k7v6Z4yeOM3PpXLq37OATU6/6BSxZ/Q0AW9K2USMunorlKgSjuq5zs4UaCMFsoVbGc2vDnwCqmqGqPlfZRCRVRGKcz8+IyCYRWSQi74vII16hvUXkWxHZIiKtnU7mEcCNTsv3xuLaKYD0/XuoGlspZz4+phI7c52e54mJjSN9/54i1x36r9e44JY2TPtiDs/cOhiAw38c4dUZk3jqlpOnvOFi1/49xMfE5cxXiYlj1/69uWL2Eu91TLxj/jHxZUb0ezhPq+TFux9jyJTXaHB7R56Z8hpD7hgcwL0oPlUqxJG2b3fO/M6M3cRXiPOJWbdtEz0v7wRAQt1GVI+rQnyM5/ipKnNemMJXY2ZxZ5di/W9xxrJf0mcJNX8LgWpOEhwnIm0LCnRO6a8HLgWuA3Kf4kepanPgQWCoqh7D06E83en7mB6QPSiAat7b2XJ3phcUU9S6w+94mJR/L6PPlT0YP+f/AHj2/0bzwHV3UKbU2Wda9WKXz+6S+xbBfI8JwvxvlxJ7bnka16mfZ/nb82bw/N2PsmHqIl64+1EeGDXUpRoHV34XZXIfn1dmTKBcmbKsGPsx9/a8lTVbN3IiMxOA9g/fRKuB1/K3p/tzT4+bufziIrsFQ4h/z/EHc8zUoF3lV9VDItIUaA1cCUzPb4ACxxXAx6p6FEBE5uRaPsv5exVQ05/ti8gAYABAterViog+NfExlfK0IqqUr1h4zL49VC5fkWPHjxe5LsANV/bguiEDeObWwSRtWsNHXy7gqckj+e3w70RIBCVLlODea251db8CoUpMHDszTrbA0zP2ULlCbN4Yr2OSnrGHShVi+firRXy6cgkLk5fz57E/OXj0MANG/oOJj77ItMWzeemexwH42xWdGPTGsGLZn0DbmbE7zxlM+gHfFv3BI4e557V/5Mxvmvo5qXs8vWm7nNh9vx1g9teLaHZhI75an0z4sNGmCqSqmaq6RFWHAgPxtELzU9RR/NP5OxM/f0mo6kRVTVDVhNjYGP8q7KeECxuSkp5K6u6fOXb8GDOXzqXbZVf5xHS7rD3vLf4IVWXlxtWUPbsMlStULHTdlJ2pOevPXbGYutXOB2Dxq++z+d0v2PzuFwz82+082icxLJIpQJO6Ddi6czupu9M4dvw4Hy6bT5cW7XxiurRox7TP56CqJG1aQ9mzz6FS+ViG3jGYH979jHXvzOftx1+mTaPmTHz0RQAqlY9l+TpPoli2ZiXnV6le3LsWEMmb13FBlZrUiKtKdFQ0vdt2Y+6KxT4x5559DtFR0QD063wDy9clc/DIYUqfVSrnLKb0WaXo0ORyNqT+WOz7cNok9C9KBa2FKiIXAlmqmv0v2hjYDlycT/hyYIKIvIinzt2ASUVs4iBwjju1PTVRkVG8ft8Qejx1F5lZmdzeqRf1a9Zh0tz3Abi72010bt6OBUlLaXBnB0qfVYoJD79Y6LoAT095hR/TfiJCIqgeV4XRDwwPxu65KioyipH3Psn1z9xLZlYmt3T8GxfVuIAp82YAcGfXG+jUrDWLkr/k0v7dKH1WScY+9GyR5b4xaChPTHiJE1mZlIwuwRsP/DVO+TOzMnlo3AjmPP82kRGRTF34ARu3p9C/ax8AJs+bRr3qtZn8yMtkZmWxaUcKia8/CUDF82KYPsRzZ0hUZCTTv5jDolVfBm1fTkeoj4cq+fVPFcuGPaf7Y4BywAkgBc8p+AfAI6qaLCKpQIKqZojIMOAmPEl3H7BEVSeJyBKv+BggWVVrikh5PDftRgMvFtaP2jShiX61cnmA9jT8/Zn5R7CrEPLKdWsU7CqEvs92rvLnXs6CNGrSUGcvm1V0IFDrnLpntK3TFcw+1FVAq3wWtfOKqen1/SuqOkxESgPLgFedGO/4DJw+VFU9ADRzudrGmCAK9RZqOD16OlFE6gMlgamq+l2wK2SMKV6WUF2iqn2DXQdjTHDZa6SNMcYFpzjAdFBYQjXGhA075TfGGNdYQjXGGFeEdjq1hGqMCSN2UcoYY1xjCdUYY1wQ3KH5/GEJ1RgTFsTdt54GRGjf1GWMMWHEWqjGmLBhp/zGGOMSS6jGGOMS60M1xpj/EdZCNcaECbttyhhjXGQJ1RhjzpgQ6unUEqoxJoyE+kUpS6jGmLBhfajGGOMaS6jGGOMCCflTfrsP1RhjXGItVGNMWPBc5Q/tFqolVGNMGLGEaowxrogI8T5US6jGmDAR+rf2W0I1xoSN0E6nllCNMWEltFOqJVRjTHgIg3dKWUI1xoSFcLhtSlQ12HUIOhHZB2wPdj28xAAZwa5EiLNjVLhQPD41VDX2dFcWkfl49ssfGara+XS3dbosoYYgEUlW1YRg1yOU2TEqnB2f4LBHT40xxiWWUI0xxiWWUEPTxGBXIAzYMSqcHZ8gsD5UY4xxibVQjTHGJZZQjTHGJZZQi5GIZIrIaq+pZgFx5UTkPj/LPORqJYuBiDwlIhtEZK1zHFqcwrqJInJbIOsXivI7ZiKyREQSnOXzRKRcEWXkxOf6vrGIdA1Q1f+n2JNSxeuoqjb2I64ccB8wLqC1CQIRaQl0B5qo6p8iEgOU8HPdKFUdH9AKhiB/jpmqnklCbAwkAPPOoAyDtVCDSkTKiMhiEflORNaJSE9n0T+B2k5LZGQhceGoMp6nWP4EUNUMVU0XkaYislREVonIAhGpDDmtqhdEZCkwWESGicgjzrLGIrLCabV9JCLnea2T3XKLEZFU53MDEfnWOa5rRaROEPb/dOR7zLwDRCTVSbSIyDMisklEFonI+9nHy9HbOQZbRKS1iJQARgA3OsflxuLaqb8kVbWpmCYgE1jtTB/hOUMo6yyLAVLwPLJcE1jvtV6+cc78oWDv1ykegzLO/m/B0wJvC0QDXwOxTsyNwBTn8xJgnNf6w4BHnM9rgbbO5xHAKK91EryOV6rzeQxws/O5BFAq2MfjdI9ZPvuZ6uxrghNbCjgH+NHreC0BXnU+dwU+cz7fAbwZ7P38K0x2yl+8fE75RSQaeEFE2gBZQDwQl896UkDc7oDX2GWqekhEmgKtgSuB6cBzwMXAImc0oUhgl9dq03OXIyLnAuVUdanz1VRgZhGb/wZ4SkSqArNU9ccz2Zfikt8xE5EnCgi/AvhYVY8CiMicXMtnOX+vwvOL27jIEmpw3QzEAk1V9bhzalryDOLCgqpm4mktLRGRdcD9wAZVbVnAKodPcRMnONmdlXOcVPU9EVkJdAMWiEh/Vf38FMsOinyO2e0FhBY1HNOfzt+Z2P9/11kfanCdC+x1kuSVQA3n+4N4TteKigs7InJhrr7LxsBGINa5+IKIRItIg8LKUdXfgF9EpLXz1a1Adms1FWjqfO7lte3zgW2qOhqYDTQ6s70pHgUcs4JGR1sO9BCRkiJSBs8vj6Lk/nkzp8kSanD9B0gQkWQ8rdBNAKq6H/hKRNaLyMiC4sJUGWCqiPwgImuB+sAQPInvJRFZg6cPsJUfZd0OjHTKaYynHxXgFeBeEfka3+HebgTWi8hqoB7w7hnvTfHI75gNyy9QVZPw/LJYg+f0Phn4rYjyvwDq20WpM2ePnhrzFyMiZZx+19LAMmCAqn4X7Hr9L7A+FGP+eiaKSH08/cdTLZkWH2uhGmOMS6wP1RhjXGIJ1RhjXGIJ1RhjXGIJ1fhFTo6UtV5EZjpXkE+3rH+JSC/n82TnAkpBse1ExJ9bqHKvl/Nsuz/f54o5pRG8vMcXMP/bLKEafx1V1caqejFwDEj0XigikadTqKr2V9UfCglph3/3pBoTdJZQzen4ErjAaT1+ISLvAetEJNIZHSvJGc3pHgDxeNO5MX0uUDG7oFwjQ3V2RtRaI57RtWriSdwPOa3j1iISKyIfOttIEpHLnXUriMhCEfleRCZQ9COYiMh/xTO61QYRGZBr2atOXRaLSKzzXW0Rme+s86WI1HPlaJq/DLsP1ZwSEYkCugDzna+aAxer6k9OUvpNVZuJyFl4nvZaCFwKXAg0xDOoyw/AlFzlxgKTgDZOWeVV9YCIjMczotYrTtx7wOuqulxEqgMLgIuAocByVR0hIt0AnwRZgDudbZQCkkTkQ+cptbOB71T17yIyxCl7IJ4X3yWq6o/iGRR7HND+NA6j+YuyhGr8Vcp5ZBM8LdS38ZyKf6uqPznfdwIaZfeP4hmDoA7QBnjfGeAjXUTyG5DkMmBZdlmqeqCAenTA85hk9nxZETnH2cZ1zrpzReQXP/ZpkIhc63yu5tR1P54RvbJHuPo3MMt5Lr4VMNNr22f5sQ3zP8QSqvFXnrcNOInFeyQoAR5Q1QW54roCRT1BIn7EgKebqmX28HS56uL3Uyoi0g5Pcm6pqkdEZAkFj+ClznZ/zX0MjPFmfajGTQvwDEoSDSAidUXkbDzPk/dx+lgr4xnTM7dvgLYiUstZt7zzfe6RkBbiOf3GiWvsfFyGZ+AYRKQLcF4RdT0X+MVJpvXwtJCzRXBylKq+eLoSfgd+EpHezjZERC4pYhvmf4wlVOOmyXj6R78TkfXABDxnQR/hGTl+HfAWJ4fZy6Gq+/D0e85yRpzKPuWeA1ybfVEKGIRn5K21IvIDJ+82GA60EZHv8HQ97CiirvOBKGf0pmeBFV7LDgMNRGQVnj7S7FGsbgbucuq3AQjnV9GYALBn+Y0xxiXWQjXGGJdYQjXGGJdYQjXGGJdYQjXGGJdYQjXGGJdYQjXGGJdYQjXGGJf8P9runYoimUUxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing confusion matrix from sklearn.metrics\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(des_tree_u, Xtest, ytest,\n",
    "cmap=plt.cm.Greens, normalize='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in above confusion matrix we can see that only slight accident predicted with good accuracy of 95% and serious and fatal with 5.9% and 6.9% respectively. 4.8% of slight accident predicted as serious, and 94% of serious accident predicted as slight and 92% of fatal accident as slight which is not good for prediction.\n",
    "\n",
    "Most of the accident predicted as slight by this model, which is a major concern to look out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Random Forest Classifier** with over sampling revealed to be the best model. Further research should be devoted to such a model, namely fine-tuning the percentage of data. This performed marginally better than the baseline. However, the F-score on the test set is very low this may be due to overfitting.\n",
    "\n",
    "Same thing goes for Linear Support Vector Machine with Oversampling here some sort of overfitting is visible and  the result of Knn is almost same on train and test data.\n",
    "\n",
    "**So Random Forest Classifier is the best model for predicting accident severity its very good in predicting slight and serious type of accident severity but not fatal type of accident, so I would not recommend this model to use it in real time based enviroment infact further research is needed to develop a model that increases its capacity to forecast serious and fatal incidents**.\n",
    "\n",
    "Further improvements in variables is required to improve accuracy of the models.\n",
    "\n",
    "## Real world Scenario\n",
    "By looking at the performance of our top model I dont think its better to deploy it in real world scenario as further improvements required to make it more accuarte.If we deploy it now than we can not fully rely on this model as if this model predict wrong than this may be result in loss of time and money "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
